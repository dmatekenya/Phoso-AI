import os
from decouple import config
from langchain_community.utilities import SQLDatabase
from langchain_experimental.sql import SQLDatabaseChain
from sqlalchemy import create_engine, MetaData
from sqlalchemy.orm import sessionmaker

from langchain.chains import create_sql_query_chain, LLMChain
from langchain_openai import ChatOpenAI, OpenAI
from langchain.prompts import ChatPromptTemplate, PromptTemplate



# OPENAI_API_KEY = config("OPENAI_API_KEY")
# DB_USER = config('DB_USER')
# DB_PASSWORD = config('DB_PASSWORD')
# DB_HOST = 'localhost'
# DB_PORT = '5432'
# DB_NAME = 'tunis_mics'
# SAMPLE_QUESTIONS = {"low-birthweight": "Which region has the highest number of children born with low birth weights?",
#                     "vaccine_rates": "Which vaccine has the lowest vaccination percentage?",
#                     "vaccine_rates_all": "What percentage of children received all vaccines before 12 months",
#                     "kids_in_sch": "Whats average percentage of children who are in preschool",
#                     "vaccines": "which vaccines did children get in Tunisia?"
#                     }
                    
# # Create the database URL
# DATABASE_URL = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'



def detect_language(text):
     # Initialize the OpenAI API
    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')

    # Create the language detection tool
    lang_detect_tool = LangDetectTool()

    # Create a Tool object
    lang_detect = Tool(
        name="Language Detection",
        func=lang_detect_tool.run,
        description="Useful for detecting the language of a given text."
    )

    # Use the tool to detect language
    return lang_detect.run(text)
                             


from langchain.utils.math import cosine_similarity
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import OpenAIEmbeddings

physics_template = """You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.

Here is a question:
{query}"""

math_template = """You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.

Here is a question:
{query}"""

embeddings = OpenAIEmbeddings()
prompt_templates = [physics_template, math_template]
prompt_embeddings = embeddings.embed_documents(prompt_templates)


def prompt_router(query):
    query_embedding = embeddings.embed_query(query)
    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]
    most_similar = prompt_templates[similarity.argmax()]
    print("Similarity results=>", most_similar)
    print("Using MATH" if most_similar == math_template else "Using PHYSICS")
    return PromptTemplate.from_template(most_similar)



prompt_router("Explain Pythagoras theorem")


def create_db_object_with_metadata():
    # Create the SQLAlchemy engine
    engine = create_engine(DATABASE_URL)
    metadata_obj = MetaData()
    metadata_obj.reflect(bind=engine)

    # Create a configured "Session" class
    Session = sessionmaker(bind=engine)
    session = Session()

    # Load custom metadata from the table_metadata and column_metadata tables
    try:
        table_metadata = session.execute("SELECT * FROM table_metadata").fetchall()
        column_metadata = session.execute("SELECT * FROM column_metadata").fetchall()

        # Add table metadata
        for row in table_metadata:
            print(row)
            table_name = row['table_name']
            description = row['description']
            table = metadata_obj.tables.get(table_name)
            table.info['description'] = description

        # Add column metadata
        for row in column_metadata:
            table_name = row['table_name']
            column_name = row['column_name']
            description = row['description']
            table = metadata_obj.tables.get(table_name)
            column = table.columns.get(column_name)
            column.info['description'] = description
    finally:
        session.close()
    db = SQLDatabase(engine=engine, metadata=metadata_obj, ignore_tables=['table_metadata', 'column_metadata'])

    return db


commodities_price = ['Maize', 'Rice', 'Soya beans', 'Beans', 'Cow peas', 'Groundnuts']
crop_estimates = ['Maize', 'Beans', 'Cow peas', 'Dolichus beans ', 'Soy beans',
       'Ground beans', 'Paprika', 'Rice', 'Pigeon peas', 'Grams',
       'Sesame ', 'Field peas', 'Velvet beans', 'Chick peas', 'Wheat',
       'Millet', 'Sorghum ', 'Groundnuts', 'Cassava', 'Sweet potatoes',
       'Potatoes', 'Tobacco', 'Flue cured', 'Sunflower ', 'Chillies',
       'Cotton ', 'Bananas', 'Mangoes', 'Oranges', 'Tangerines', 'Coffee',
       'Pineapples', 'Guava', 'Pawpaws', 'Peaches', 'Lemons',
       'Grape fruits', 'Apples', 'Avocado pear', 'Macademia', 'Tomatoes',
       'Onions', 'Cabbage', 'Egg plants', 'Okra', 'Cucumber']
price_estimates_key_words = ["price", "cheap", "produce", 
                                 "buy", "sell", "sale", "find"]
all_kw = [i.lower() for i in set(commodities_price+crop_estimates+price_estimates_key_words)]
print(all_kw)


import os
import json
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain

def load_examples(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)



def format_translation_examples(examples_file, source_language, target_language):
    examples = load_examples(examples_file)
    key = f"{source_language}-{target_language}"
    if key in examples:
        return "\n".join([f"{source_language}: {ex[source_language]}\n{target_language}: {ex[target_language]}" 
                          for ex in examples[key]])
    return ""


def translate_with_openai(text, src_lan, dest_lan):
    # Create a ChatOpenAI instance
    chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)
    
    # Get and format translation examples
    formated_examples = format_translation_examples("./translation_examples.json", source_language=src_lan, 
                                target_language=dest_lan)
    # Create a system message with examples
    system_template = """You are a professional translator. Your task is to translate {src_lan} to {dest_lan}.
        Here are a few examples:

        {examples}

        Now, translate the following text:"""

    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

    # Create a human message for the actual translation request
    human_template = "{text}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    # Combine the prompts
    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    # Create an LLMChain for translation
    translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)
    
    return translation_chain.run({
        "source_language": src_lan,
        "target_language": dest_lan,
        "examples": formated_examples,
        "text": text
    })




chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)
# Create a system message with examples
system_template = """You are a professional translator. Your task is to translate {source_language} to {target_language}.
Here are a few examples:

{examples}

Now, translate the following text:"""

system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

# Create a human message for the actual translation request
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# Combine the prompts
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Create an LLMChain for translation
translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)

# Function to translate text
def translate_text(text, source_language, target_language):
    formated_examples = format_translation_examples("./translation_examples.json", source_language, 
                                target_language)
    return translation_chain.run({
        "source_language": source_language,
        "target_language": target_language,
        "examples": formated_examples,
        "text": text
    })


translate_text("Mpunga ukugulitsidwa pa mtengo wanji?", "Chichewa", "English")


format_translation_examples(examples_file="./translation_examples.json", 
                            source_language="Chichewa", target_language="English")


format_examples(examples, source_language="Chichewa", target_language="English")


relevant_examples = examples["Chichewa-English"]

for item in relevant_examples:
    print(item)
    "\n".join([f"{source_language}: {ex['source']}\n{target_language}: {ex['target']}" 
                          for ex in examples[key]])




# Create a ChatOpenAI instance
chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)

# Create a system message with examples
system_template = """You are a professional translator. Your task is to translate {source_language} to {target_language}.
Here are a few examples:

{source_language}: Mtedza ukugulitsidwa pabwanji?
{target_language}: Whats the price of groundnuts?

{source_language}: Chimanga chikupezeka kuti?
{target_language}: Where can I find maize?

{source_language}: Ndikuti nyemba zikutchipa?
{target_language}: Where can I find beans at cheap price?

{source_language}: Chimanga chili pabwanji pano?
{target_language}: Whats the price of maize now?

{source_language}: Ku Dowa chimanga chili pa bwanji?
{target_language}: Whats the price of maize in Dowa?

{source_language}: Kodi ndi boma liti anakolola chimanga chambiri pakati pa Lilongwe kapena Kasungu?
{target_language}: Which district produced more maize: Lilongwe or Kasungu?

{source_language}: Kodi chimanga chili pa bwanji ku Rumphi?
{target_language}: How much is maize per Kg in Rumphi?

{source_language}: Mpunga ukugulitsidwa ndalama zingati ku Lilongwe?
{target_language}: Whats the price of rice in Lilongwe?

{source_language}: Mtedza otchipa ukupezeka mboma liti?
{target_language}: Which district has the cheap price for groundnuts?

{source_language}: Chimanga chambiri chikupezeka kuti?
{target_language}: Where can I find maize?

{source_language}: Ndi boma liti komwe anakolola chimanga chambiri?
{target_language}: Which district harvested large quantities of maize?

{source_language}: Ndi mbeu zanji anakolola bwino ku Rumphi?
{target_language}: Which crops produced the most yields in Rumphi

{source_language}: Soya ali pabwanji?
{target_language}: Whats the price of soya?

{source_language}: Mtedza otchipa ndingaupeze kuti?
{target_language}: Where can I find groundnuts at reasonable price?


Now, translate the following text:"""

system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

# Create a human message for the actual translation request
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# Combine the prompts
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Create an LLMChain for translation
translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)

# Function to translate text
def translate_text(text, source_language, target_language):
    return translation_chain.run({
        "source_language": source_language,
        "target_language": target_language,
        "text": text
    })

# Example usage
source_text = "Mtedza ndingaupeze kuti?"
source_language = "Chichewa"
target_language = "English"

translated_text = translate_text(source_text, source_language, target_language)

print(f"{source_language}: {source_text}")
print(f"{target_language}: {translated_text}")


# Create an OpenAI instance
llm = OpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY, model="gpt-4")

# Create a prompt template for translation
translation_template = PromptTemplate(
    input_variables=["source_language", "target_language", "text"],
    template="Translate the following {source_language} text to {target_language}: {text}"
)

# Create an LLMChain for translation
translation_chain = LLMChain(llm=llm, prompt=translation_template)

# Function to translate text
def translate_text(text, source_language, target_language):
    return translation_chain.run({
        "source_language": source_language,
        "target_language": target_language,
        "text": text
    })



source_text = "Chimanga chili pa bwanji ku Malawi?"
source_language = "Chichewa"
target_language = "English"

translated_text = translate_text(source_text, source_language, target_language)
print(translated_text)


translated_text


from langchain.llms import OpenAI


def translate_text(text, source_language="English", target_language="Chichewa"):

    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')
    # Create a template for the translation
    translation_template = ChatPromptTemplate.from_template(
    "Translate the following {source_language} text to {target_language}: {text}"
)

    # Create a chain with the LLM and the translation template
    translation_chain = LLMChain(llm=llm, prompt=translation_template)

    translation = translation_chain.run({
        'text': text,
        'source_language': source_language,
        'target_language': target_language
    })
    return translation






translated_text = translate_text(text="cheap")


!pip install googletrans==4.0.0-rc1


import openai
def translate_text(text, source_language="English", target_language="Chichewa"):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": f"You are a helpful assistant that translates {source_language} to {target_language}."
            },
            {
                "role": "user",
                "content": f"Translate the following text from {source_language} to {target_language}:\n\n{text}"
            }
        ]
    )
    translation = response['choices'][0]['message']['content']
    return translation.strip()


translate_text(text="cheap", source_language="English", target_language="Chichewa")


from googletrans import Translator
def translate_text(text, source_language="en", target_language="ny"):
    translator = Translator()
    translation = translator.translate(text, src=source_language, dest=target_language)
    return translation.text


db = create_db_object_with_metadata()
# llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
# chain = create_sql_query_chain(llm, db)
# response = chain.invoke({"question": "{}".format(SAMPLE_QUESTIONS["low-birthweight"])})
# response


import sqlalchemy


sqlalchemy.__version__



examples = [
    {"input": "Which region has the highest number of children born with low birth weights?", 
     "query": "SELECT * FROM tab4711 ORDER BY number_children DESC LIMIT 1;",},

     {"input": "Which region has the highest percentage of children born with low birth weights?", 
     "query": "SELECT * FROM tab4711 ORDER BY percentage_below_2500g DESC LIMIT 1;",
     },

     {"input": "How many children received all vaccines before 12 months?", 
     "query": "SELECT vacc_b4_12months FROM tab501 WHERE vacc_category = 'All vaccinations';"},

     {"input": "Which region has the lowest rates in preschool for children?", 
     "query": "SELECT * FROM tab9011 ORDER BY percentage_children_sch ASC LIMIT 1;",},

     {"input": "Whats the average literacy rate among young women in Tunisia?",
      "query": "SELECT AVG(percentage_literate) AS avg_literacy_rate FROM tab971;",},
]


from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool

execute_query = QuerySQLDataBaseTool(db=db)
write_query = create_sql_query_chain(llm, db)


from operator import itemgetter

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

answer_prompt = PromptTemplate.from_template(
    """Given the following user question, corresponding SQL query, and SQL result, answer the user question.

Question: {question}
SQL Query: {query}
SQL Result: {result}
Answer: """
)

answer = answer_prompt | llm | StrOutputParser()
chain = (
    RunnablePassthrough.assign(query=write_query).assign(
        result=itemgetter("query") | execute_query
    )
    | answer
)

chain.invoke({"question": "{}".format(SAMPLE_QUESTIONS['vaccine_rates_all'])})


from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

example_prompt = PromptTemplate.from_template("User input: {input}\nSQL query: {query}")
sql_prompt = FewShotPromptTemplate(
    examples=examples[:5],
    example_prompt=example_prompt,
    prefix="You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. Unless otherwise specificed, do not return more than {top_k} rows.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries.",
    suffix="User input: {input}\nSQL query: ",
    input_variables=["input", "top_k", "table_info"],
)


from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool

execute_query = QuerySQLDataBaseTool(db=db, verbose=True)
write_query = create_sql_query_chain(llm, db, sql_prompt)


answer_prompt = PromptTemplate.from_template(
    """Given the following user question, corresponding SQL query, and SQL result, answer the user question.

Question: {question}
SQL Query: {query}
SQL Result: {result}
Answer: """
)

answer = answer_prompt | llm | StrOutputParser()
chain = (
    RunnablePassthrough.assign(query=write_query).assign(
        result=itemgetter("query") | execute_query
    )
    | answer
)

chain.invoke({"question": "{}".format(SAMPLE_QUESTIONS["vaccines"])})



