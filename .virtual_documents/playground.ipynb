from decouple import config
from langchain_community.utilities import SQLDatabase
from langchain_experimental.sql import SQLDatabaseChain
from sqlalchemy import create_engine, MetaData
from sqlalchemy.orm import sessionmaker

from langchain.chains import create_sql_query_chain, LLMChain
from langchain_openai import ChatOpenAI, OpenAI
from langchain.prompts import ChatPromptTemplate, PromptTemplate



# OPENAI_API_KEY = config("OPENAI_API_KEY")
# DB_USER = config('DB_USER')
# DB_PASSWORD = config('DB_PASSWORD')
# DB_HOST = 'localhost'
# DB_PORT = '5432'
# DB_NAME = 'tunis_mics'
# SAMPLE_QUESTIONS = {"low-birthweight": "Which region has the highest number of children born with low birth weights?",
#                     "vaccine_rates": "Which vaccine has the lowest vaccination percentage?",
#                     "vaccine_rates_all": "What percentage of children received all vaccines before 12 months",
#                     "kids_in_sch": "Whats average percentage of children who are in preschool",
#                     "vaccines": "which vaccines did children get in Tunisia?"
#                     }
                    
# # Create the database URL
# DATABASE_URL = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'


prompt = """Answer the question based on the context below. If the
question cannot be answered using the information provided answer
with "I don't know".

Context: Large Language Models (LLMs) are the latest models used in NLP.
Their superior performance over smaller models has made them incredibly
useful for developers building NLP enabled applications. These models
can be accessed via Hugging Face's `transformers` library, via OpenAI
using the `openai` library, and via Cohere using the `cohere` library.

Question: Which libraries and model providers offer LLMs?

Answer: """



from langchain_openai import OpenAI

# initialize the models
openai = OpenAI(openai_api_key=OPENAI_API_KEY)



print(openai(prompt))



from langchain_core.prompts import PromptTemplate

template = """Answer the question based on the context below. If the
question cannot be answered using the information provided answer
with "I don't know".

Context: Large Language Models (LLMs) are the latest models used in NLP.
Their superior performance over smaller models has made them incredibly
useful for developers building NLP enabled applications. These models
can be accessed via Hugging Face's `transformers` library, via OpenAI
using the `openai` library, and via Cohere using the `cohere` library.

Question: {query}

Answer: """

prompt_template = PromptTemplate(
    input_variables=["query"],
    template=template
)


openai(formatted_prompt)


formatted_prompt = prompt_template.format(query="Which libraries and model providers offer LLMs?")


type(prompt_template)


prompt = """The following is a conversation with an AI assistant.
The assistant is typically sarcastic and witty, producing creative 
and funny responses to the users questions. Here are some examples: 

User: What is the meaning of life?
AI: """

openai.temperature = 1.0  # increase creativity/randomness of output

print(openai(prompt))



prompt = """The following are exerpts from conversations with an AI
assistant. The assistant is typically sarcastic and witty, producing
creative  and funny responses to the users questions. Here are some
examples: 

User: How are you?
AI: I can't complain but sometimes I still do.

User: What time is it?
AI: It's time to get a watch.

User: What is the meaning of life?
AI: """

print(openai(prompt))



from langchain import FewShotPromptTemplate

# create our examples
examples = [
    {
        "query": "How are you?",
        "answer": "I can't complain but sometimes I still do."
    }, {
        "query": "What time is it?",
        "answer": "It's time to get a watch."
    }
]

# create a example template
example_template = """
User: {query}
AI: {answer}
"""

# create a prompt example from above template
example_prompt = PromptTemplate(
    input_variables=["query", "answer"],
    template=example_template
)

# now break our previous prompt into a prefix and suffix
# the prefix is our instructions
prefix = """The following are exerpts from conversations with an AI
assistant. The assistant is typically sarcastic and witty, producing
creative  and funny responses to the users questions. Here are some
examples: 
"""
# and the suffix our user input and output indicator
suffix = """
User: {query}
AI: """

# now create the few shot prompt template
few_shot_prompt_template = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=prefix,
    suffix=suffix,
    input_variables=["query"],
    example_separator="\n\n"
)



query = "What is the meaning of life?"

print(few_shot_prompt_template.format(query=query))



openai(few_shot_prompt_template.format(query=query))


from langchain.chat_models import ChatOpenAI
from langchain_core.tools import Tool
from langchain_core.tools import langdetect


!pip install pandas openai


import pandas as pd
from openai import OpenAI
import os

# Define two variants of the prompt to test zero-shot
# vs few-shot
prompt_A = """Product description: A pair of shoes that can
fit any foot size.
Seed words: adaptable, fit, omni-fit.
Product names:"""

prompt_B = """Product description: A home milkshake maker.
Seed words: fast, healthy, compact.
Product names: HomeShaker, Fit Shaker, QuickShake, Shake
Maker

Product description: A watch that can tell accurate time in
space.
Seed words: astronaut, space-hardened, eliptical orbit
Product names: AstroTime, SpaceGuard, Orbit-Accurate,
EliptoTime.

Product description: A pair of shoes that can fit any foot
size.
Seed words: adaptable, fit, omni-fit.
Product names:"""

test_prompts = [prompt_A, prompt_B]


# Set your OpenAI key as an environment variable
# https://platform.openai.com/api-keys
client = OpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  # Default
)

def get_response(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    )
    return response.choices[0].message.content

# Iterate through the prompts and get responses
responses = []
num_tests = 5

for idx, prompt in enumerate(test_prompts):
    # prompt number as a letter
    var_name = chr(ord('A') + idx)

    for i in range(num_tests):
        # Get a response from the model
        response = get_response(prompt)

        data = {
            "variant": var_name,
            "prompt": prompt,
            "response": response
            }
        responses.append(data)

# Convert responses into a dataframe
df = pd.DataFrame(responses)

# Save the dataframe as a CSV file
df.to_csv("responses.csv", index=False)

print(df)


df


!pip install ipywidgets


import ipywidgets as widgets
from IPython.display import display
import pandas as pd

# load the responses.csv file
df = pd.read_csv("responses.csv")

# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)

# df is your dataframe and 'response' is the column with the
# text you want to test
response_index = 0
# add a new column to store feedback
df['feedback'] = pd.Series(dtype='str')


def update_response():
    new_response = df.iloc[response_index]['response']
    if pd.notna(new_response):
        new_response = "<p>" + new_response + "</p>"
    else:
        new_response = "<p>No response</p>"
    response.value = new_response
    count_label.value = f"Response: {response_index + 1}"
    count_label.value += f"/{len(df)}"



def on_button_clicked(b):
    global response_index
    #  convert thumbs up / down to 1 / 0
    user_feedback = 1 if b.description == "\U0001F44D" else 0

    # update the feedback column
    df.at[response_index, 'feedback'] = user_feedback

    response_index += 1
    if response_index < len(df):
        update_response()
    else:
        # save the feedback to a CSV file
        df.to_csv("results.csv", index=False)

        print("A/B testing completed. Here's the results:")
        # Calculate score and num rows for each variant
        summary_df = df.groupby('variant').agg(
            count=('feedback', 'count'),
            score=('feedback', 'mean')).reset_index()
        print(summary_df)



response = widgets.HTML()
count_label = widgets.Label()

update_response()

thumbs_up_button = widgets.Button(description='\U0001F44D')
thumbs_up_button.on_click(on_button_clicked)

thumbs_down_button = widgets.Button(
    description='\U0001F44E')
thumbs_down_button.on_click(on_button_clicked)

button_box = widgets.HBox([thumbs_down_button,
thumbs_up_button])

display(response, button_box, count_label)


import ipywidgets as widgets
from IPython.display import display
import pandas as pd

# load the responses.csv file
df = pd.read_csv("responses.csv")

# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)

# df is your dataframe and 'response' is the column with the
# text you want to test
response_index = 0
# add a new column to store feedback
df['feedback'] = pd.Series(dtype='str')

def on_button_clicked(b):
    global response_index
    #  convert thumbs up / down to 1 / 0
    user_feedback = 1 if b.description == "\U0001F44D" else 0

    # update the feedback column
    df.at[response_index, 'feedback'] = user_feedback

    response_index += 1
    if response_index < len(df):
        update_response()
    else:
        # save the feedback to a CSV file
        df.to_csv("results.csv", index=False)

        print("A/B testing completed. Here's the results:")
        # Calculate score and num rows for each variant
        summary_df = df.groupby('variant').agg(
            count=('feedback', 'count'),
            score=('feedback', 'mean')).reset_index()
        print(summary_df)

def update_response():
    new_response = df.iloc[response_index]['response']
    if pd.notna(new_response):
        new_response = "<p>" + new_response + "</p>"
    else:
        new_response = "<p>No response</p>"
    response.value = new_response
    count_label.value = f"Response: {response_index + 1}"
    count_label.value += f"/{len(df)}"

response = widgets.HTML()
count_label = widgets.Label()

update_response()

thumbs_up_button = widgets.Button(description='\U0001F44D')
thumbs_up_button.on_click(on_button_clicked)

thumbs_down_button = widgets.Button(
    description='\U0001F44E')
thumbs_down_button.on_click(on_button_clicked)

button_box = widgets.HBox([thumbs_down_button,
thumbs_up_button])


display(response, button_box, count_label)


from langchain.tools import LangDetectTool


!pip install langdetect


def detect_language(text):
     # Initialize the OpenAI API
    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')

    # Create the language detection tool
    lang_detect_tool = LangDetectTool()

    # Create a Tool object
    lang_detect = Tool(
        name="Language Detection",
        func=lang_detect_tool.run,
        description="Useful for detecting the language of a given text."
    )

    # Use the tool to detect language
    return lang_detect.run(text)
                             


from langchain.utils.math import cosine_similarity
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import OpenAIEmbeddings

physics_template = """You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.

Here is a question:
{query}"""

math_template = """You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.

Here is a question:
{query}"""

embeddings = OpenAIEmbeddings()
prompt_templates = [physics_template, math_template]
prompt_embeddings = embeddings.embed_documents(prompt_templates)


def prompt_router(query):
    query_embedding = embeddings.embed_query(query)
    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]
    most_similar = prompt_templates[similarity.argmax()]
    print("Similarity results=>", most_similar)
    print("Using MATH" if most_similar == math_template else "Using PHYSICS")
    return PromptTemplate.from_template(most_similar)



prompt_router("Explain Pythagoras theorem")


def create_db_object_with_metadata():
    # Create the SQLAlchemy engine
    engine = create_engine(DATABASE_URL)
    metadata_obj = MetaData()
    metadata_obj.reflect(bind=engine)

    # Create a configured "Session" class
    Session = sessionmaker(bind=engine)
    session = Session()

    # Load custom metadata from the table_metadata and column_metadata tables
    try:
        table_metadata = session.execute("SELECT * FROM table_metadata").fetchall()
        column_metadata = session.execute("SELECT * FROM column_metadata").fetchall()

        # Add table metadata
        for row in table_metadata:
            print(row)
            table_name = row['table_name']
            description = row['description']
            table = metadata_obj.tables.get(table_name)
            table.info['description'] = description

        # Add column metadata
        for row in column_metadata:
            table_name = row['table_name']
            column_name = row['column_name']
            description = row['description']
            table = metadata_obj.tables.get(table_name)
            column = table.columns.get(column_name)
            column.info['description'] = description
    finally:
        session.close()
    db = SQLDatabase(engine=engine, metadata=metadata_obj, ignore_tables=['table_metadata', 'column_metadata'])

    return db


commodities_price = ['Maize', 'Rice', 'Soya beans', 'Beans', 'Cow peas', 'Groundnuts']
crop_estimates = ['Maize', 'Beans', 'Cow peas', 'Dolichus beans ', 'Soy beans',
       'Ground beans', 'Paprika', 'Rice', 'Pigeon peas', 'Grams',
       'Sesame ', 'Field peas', 'Velvet beans', 'Chick peas', 'Wheat',
       'Millet', 'Sorghum ', 'Groundnuts', 'Cassava', 'Sweet potatoes',
       'Potatoes', 'Tobacco', 'Flue cured', 'Sunflower ', 'Chillies',
       'Cotton ', 'Bananas', 'Mangoes', 'Oranges', 'Tangerines', 'Coffee',
       'Pineapples', 'Guava', 'Pawpaws', 'Peaches', 'Lemons',
       'Grape fruits', 'Apples', 'Avocado pear', 'Macademia', 'Tomatoes',
       'Onions', 'Cabbage', 'Egg plants', 'Okra', 'Cucumber']
price_estimates_key_words = ["price", "cheap", "produce", 
                                 "buy", "sell", "sale", "find"]
all_kw = [i.lower() for i in set(commodities_price+crop_estimates+price_estimates_key_words)]
print(all_kw)


import os
import json
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain

def load_examples(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)



def format_translation_examples(examples_file, source_language, target_language):
    examples = load_examples(examples_file)
    key = f"{source_language}-{target_language}"
    if key in examples:
        return "\n".join([f"{source_language}: {ex[source_language]}\n{target_language}: {ex[target_language]}" 
                          for ex in examples[key]])
    return ""


def translate_with_openai(text, src_lan, dest_lan):
    # Create a ChatOpenAI instance
    chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)
    
    # Get and format translation examples
    formated_examples = format_translation_examples("./translation_examples.json", source_language=src_lan, 
                                target_language=dest_lan)
    # Create a system message with examples
    system_template = """You are a professional translator. Your task is to translate {src_lan} to {dest_lan}.
        Here are a few examples:

        {examples}

        Now, translate the following text:"""

    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

    # Create a human message for the actual translation request
    human_template = "{text}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    # Combine the prompts
    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    # Create an LLMChain for translation
    translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)
    
    return translation_chain.run({
        "source_language": src_lan,
        "target_language": dest_lan,
        "examples": formated_examples,
        "text": text
    })




chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)
# Create a system message with examples
system_template = """You are a professional translator. Your task is to translate {source_language} to {target_language}.
Here are a few examples:

{examples}

Now, translate the following text:"""

system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

# Create a human message for the actual translation request
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# Combine the prompts
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Create an LLMChain for translation
translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)

# Function to translate text
def translate_text(text, source_language, target_language):
    formated_examples = format_translation_examples("./translation_examples.json", source_language, 
                                target_language)
    return translation_chain.run({
        "source_language": source_language,
        "target_language": target_language,
        "examples": formated_examples,
        "text": text
    })


translate_text("Mpunga ukugulitsidwa pa mtengo wanji?", "Chichewa", "English")


format_translation_examples(examples_file="./translation_examples.json", 
                            source_language="Chichewa", target_language="English")


format_examples(examples, source_language="Chichewa", target_language="English")


relevant_examples = examples["Chichewa-English"]

for item in relevant_examples:
    print(item)
    "\n".join([f"{source_language}: {ex['source']}\n{target_language}: {ex['target']}" 
                          for ex in examples[key]])




# Create a ChatOpenAI instance
chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)

# Create a system message with examples
system_template = """You are a professional translator. Your task is to translate {source_language} to {target_language}.
Here are a few examples:

{source_language}: Mtedza ukugulitsidwa pabwanji?
{target_language}: Whats the price of groundnuts?

{source_language}: Chimanga chikupezeka kuti?
{target_language}: Where can I find maize?

{source_language}: Ndikuti nyemba zikutchipa?
{target_language}: Where can I find beans at cheap price?

{source_language}: Chimanga chili pabwanji pano?
{target_language}: Whats the price of maize now?

{source_language}: Ku Dowa chimanga chili pa bwanji?
{target_language}: Whats the price of maize in Dowa?

{source_language}: Kodi ndi boma liti anakolola chimanga chambiri pakati pa Lilongwe kapena Kasungu?
{target_language}: Which district produced more maize: Lilongwe or Kasungu?

{source_language}: Kodi chimanga chili pa bwanji ku Rumphi?
{target_language}: How much is maize per Kg in Rumphi?

{source_language}: Mpunga ukugulitsidwa ndalama zingati ku Lilongwe?
{target_language}: Whats the price of rice in Lilongwe?

{source_language}: Mtedza otchipa ukupezeka mboma liti?
{target_language}: Which district has the cheap price for groundnuts?

{source_language}: Chimanga chambiri chikupezeka kuti?
{target_language}: Where can I find maize?

{source_language}: Ndi boma liti komwe anakolola chimanga chambiri?
{target_language}: Which district harvested large quantities of maize?

{source_language}: Ndi mbeu zanji anakolola bwino ku Rumphi?
{target_language}: Which crops produced the most yields in Rumphi

{source_language}: Soya ali pabwanji?
{target_language}: Whats the price of soya?

{source_language}: Mtedza otchipa ndingaupeze kuti?
{target_language}: Where can I find groundnuts at reasonable price?


Now, translate the following text:"""

system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

# Create a human message for the actual translation request
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# Combine the prompts
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Create an LLMChain for translation
translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)

# Function to translate text
def translate_text(text, source_language, target_language):
    return translation_chain.run({
        "source_language": source_language,
        "target_language": target_language,
        "text": text
    })

# Example usage
source_text = "Mtedza ndingaupeze kuti?"
source_language = "Chichewa"
target_language = "English"

translated_text = translate_text(source_text, source_language, target_language)

print(f"{source_language}: {source_text}")
print(f"{target_language}: {translated_text}")


# Create an OpenAI instance
llm = OpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY, model="gpt-4")

# Create a prompt template for translation
translation_template = PromptTemplate(
    input_variables=["source_language", "target_language", "text"],
    template="Translate the following {source_language} text to {target_language}: {text}"
)

# Create an LLMChain for translation
translation_chain = LLMChain(llm=llm, prompt=translation_template)

# Function to translate text
def translate_text(text, source_language, target_language):
    return translation_chain.run({
        "source_language": source_language,
        "target_language": target_language,
        "text": text
    })



source_text = "Chimanga chili pa bwanji ku Malawi?"
source_language = "Chichewa"
target_language = "English"

translated_text = translate_text(source_text, source_language, target_language)
print(translated_text)


translated_text


from langchain.llms import OpenAI


def translate_text(text, source_language="English", target_language="Chichewa"):

    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')
    # Create a template for the translation
    translation_template = ChatPromptTemplate.from_template(
    "Translate the following {source_language} text to {target_language}: {text}"
)

    # Create a chain with the LLM and the translation template
    translation_chain = LLMChain(llm=llm, prompt=translation_template)

    translation = translation_chain.run({
        'text': text,
        'source_language': source_language,
        'target_language': target_language
    })
    return translation






translated_text = translate_text(text="cheap")


!pip install googletrans==4.0.0-rc1


import openai
def translate_text(text, source_language="English", target_language="Chichewa"):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": f"You are a helpful assistant that translates {source_language} to {target_language}."
            },
            {
                "role": "user",
                "content": f"Translate the following text from {source_language} to {target_language}:\n\n{text}"
            }
        ]
    )
    translation = response['choices'][0]['message']['content']
    return translation.strip()


translate_text(text="cheap", source_language="English", target_language="Chichewa")


from googletrans import Translator
def translate_text(text, source_language="en", target_language="ny"):
    translator = Translator()
    translation = translator.translate(text, src=source_language, dest=target_language)
    return translation.text


db = create_db_object_with_metadata()
# llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
# chain = create_sql_query_chain(llm, db)
# response = chain.invoke({"question": "{}".format(SAMPLE_QUESTIONS["low-birthweight"])})
# response


import sqlalchemy


sqlalchemy.__version__



examples = [
    {"input": "Which region has the highest number of children born with low birth weights?", 
     "query": "SELECT * FROM tab4711 ORDER BY number_children DESC LIMIT 1;",},

     {"input": "Which region has the highest percentage of children born with low birth weights?", 
     "query": "SELECT * FROM tab4711 ORDER BY percentage_below_2500g DESC LIMIT 1;",
     },

     {"input": "How many children received all vaccines before 12 months?", 
     "query": "SELECT vacc_b4_12months FROM tab501 WHERE vacc_category = 'All vaccinations';"},

     {"input": "Which region has the lowest rates in preschool for children?", 
     "query": "SELECT * FROM tab9011 ORDER BY percentage_children_sch ASC LIMIT 1;",},

     {"input": "Whats the average literacy rate among young women in Tunisia?",
      "query": "SELECT AVG(percentage_literate) AS avg_literacy_rate FROM tab971;",},
]


from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool

execute_query = QuerySQLDataBaseTool(db=db)
write_query = create_sql_query_chain(llm, db)


from operator import itemgetter

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

answer_prompt = PromptTemplate.from_template(
    """Given the following user question, corresponding SQL query, and SQL result, answer the user question.

Question: {question}
SQL Query: {query}
SQL Result: {result}
Answer: """
)

answer = answer_prompt | llm | StrOutputParser()
chain = (
    RunnablePassthrough.assign(query=write_query).assign(
        result=itemgetter("query") | execute_query
    )
    | answer
)

chain.invoke({"question": "{}".format(SAMPLE_QUESTIONS['vaccine_rates_all'])})


from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

example_prompt = PromptTemplate.from_template("User input: {input}\nSQL query: {query}")
sql_prompt = FewShotPromptTemplate(
    examples=examples[:5],
    example_prompt=example_prompt,
    prefix="You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. Unless otherwise specificed, do not return more than {top_k} rows.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries.",
    suffix="User input: {input}\nSQL query: ",
    input_variables=["input", "top_k", "table_info"],
)


from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool

execute_query = QuerySQLDataBaseTool(db=db, verbose=True)
write_query = create_sql_query_chain(llm, db, sql_prompt)


answer_prompt = PromptTemplate.from_template(
    """Given the following user question, corresponding SQL query, and SQL result, answer the user question.

Question: {question}
SQL Query: {query}
SQL Result: {result}
Answer: """
)

answer = answer_prompt | llm | StrOutputParser()
chain = (
    RunnablePassthrough.assign(query=write_query).assign(
        result=itemgetter("query") | execute_query
    )
    | answer
)

chain.invoke({"question": "{}".format(SAMPLE_QUESTIONS["vaccines"])})



