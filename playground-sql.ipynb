{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chains import create_sql_query_chain, LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.evaluation import load_evaluator, EmbeddingDistance\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = config(\"OPENAI_API_KEY\")\n",
    "DB_USER = config('DB_USER')\n",
    "DB_PASSWORD = config('DB_PASSWORD')\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '5432'\n",
    "DB_NAME = 'food_security'\n",
    "DATABASE_URL = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "FILE_SQL_EXAMPLES_EN = \"sql_examples_en.json\"\n",
    "USE_BEST_MATCHING_COLUMNS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt to Select best Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_database(database_url=DATABASE_URL):\n",
    "    \"\"\"Connects to a postgreSQL\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    database_url : String\n",
    "        postgreSQL database connection URL, by default DATABASE_URL\n",
    "    \"\"\"\n",
    "    # conn = psycopg2.connect(f\"dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD}\")\n",
    "    conn = psycopg2.connect(database_url)\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Query to get table names and column names\n",
    "    cur.execute(\"SELECT table_name, description FROM table_metadata\")\n",
    "    tables = cur.fetchall()\n",
    "\n",
    "    cur.execute(\"SELECT table_name, column_name, description FROM column_metadata\")\n",
    "    columns = cur.fetchall()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return tables, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_table_prompt(user_query, tables, columns, \n",
    "                           return_chain=True, llm=None):# Define the template for selecting the best table\n",
    "    template = \"\"\"\n",
    "    You are a database assistant. Given the following tables and columns with their descriptions, select the best table that matches the user's query.\n",
    "\n",
    "    Tables and Columns:\n",
    "    {table_info}\n",
    "\n",
    "    User Query:\n",
    "    {user_query}\n",
    "\n",
    "    Provide the output in the following JSON format:\n",
    "    {{\n",
    "        \"best_matching_table\": {{\n",
    "            \"table_name\": \"<best_table_name>\",\n",
    "            \"description\": \"<best_table_description>\"\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # Prepare the table_info string including descriptions for each table and its columns\n",
    "    table_info = \"\"\n",
    "    for table in tables:\n",
    "        table_name, table_description = table\n",
    "        table_info += f\"Table: {table_name} - {table_description}\\n\"\n",
    "        table_columns = [col for col in columns if col[0] == table_name]\n",
    "        for column in table_columns:\n",
    "            _, column_name, column_description = column\n",
    "            table_info += f\"    Column: {column_name} - {column_description}\\n\"\n",
    "        table_info += \"\\n\"\n",
    "\n",
    "    # Create the PromptTemplate\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"table_info\", \"user_query\"]\n",
    "    )\n",
    "\n",
    "    # Format the template \n",
    "    formatted_prompt = prompt_template.format(table_info=table_info, user_query=user_query)\n",
    "\n",
    "    if return_chain:\n",
    "        # Create the chain using the ChatOpenAI model and the PromptTemplate\n",
    "        chain = LLMChain(llm=llm,prompt=prompt_template)\n",
    "        return chain, {\"table_info\": table_info, \"user_query\": user_query}\n",
    "\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_info(table_name, columns):\n",
    "    columns_info = \"\"\n",
    "    for column in columns:\n",
    "        table, column_name, column_description = column\n",
    "        if table == table_name:\n",
    "            columns_info += f\"    Column: {column_name} - {column_description}\\n\"\n",
    "    return columns_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_columns_prompt(user_query, best_matching_table, columns, \n",
    "                       return_chain=True, llm=None):\n",
    "    # Define the template for selecting the relevant columns\n",
    "    column_template = \"\"\"\n",
    "    You are a database assistant. Given the following columns for the table '{table_name}', select the columns that are most relevant to the user's query.\n",
    "\n",
    "    Table Description: {table_description}\n",
    "\n",
    "    Columns:\n",
    "    {columns_info}\n",
    "\n",
    "    User Query:\n",
    "    {user_query}\n",
    "\n",
    "    Relevant Columns:\n",
    "    \"\"\"\n",
    "\n",
    "    columns_info = get_columns_info(best_matching_table[\"table_name\"], columns)\n",
    "\n",
    "    # Create the PromptTemplate for column selection\n",
    "    column_prompt_template = PromptTemplate(\n",
    "        template=column_template,\n",
    "        input_variables=[\"table_name\", \"table_description\", \"columns_info\", \"user_query\"]\n",
    "    )\n",
    "\n",
    "    # Example usage of the template with a user query\n",
    "    formatted_column_prompt = column_prompt_template.format(\n",
    "        table_name=best_matching_table[\"table_name\"],\n",
    "        table_description=best_matching_table[\"description\"],\n",
    "        columns_info=columns_info,\n",
    "        user_query=user_query\n",
    "    )\n",
    "\n",
    "    # Prepare the context for running the chain\n",
    "    context = {\n",
    "        \"table_name\": best_matching_table[\"table_name\"],\n",
    "        \"table_description\": best_matching_table[\"description\"],\n",
    "        \"columns_info\": columns_info,\n",
    "        \"user_query\": user_query}\n",
    "\n",
    "    if return_chain:\n",
    "        chain = LLMChain(llm=llm,prompt=column_prompt_template)\n",
    "        return chain, context\n",
    "\n",
    "    return formatted_column_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sql_examples(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_prompt(examples, best_matching_table, columns_metadata, \n",
    "                      use_best_matching_columns=False, llm=None):\n",
    "    \"\"\"\n",
    "    Creates a FewShotPromptTemplate for generating SQL queries based on table and column metadata.\n",
    "\n",
    "    This function generates a prompt template that includes detailed information about the table and its columns.\n",
    "    The generated prompt instructs a language model (LLM) to create a syntactically correct SQL query based on\n",
    "    user input. If the table contains a date column and the user does not specify a date, the prompt also instructs\n",
    "    the LLM to retrieve the most recent data available.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : list of dict\n",
    "        A list of example inputs and corresponding SQL queries. Each example should be a dictionary with 'input' and 'query' keys.\n",
    "    best_matching_table : dict\n",
    "        A dictionary containing the best matching table information with 'table_name' and 'description' keys.\n",
    "    columns_metadata : list of tuples\n",
    "        A list of tuples containing columns metadata. Each tuple should include 'table_name', 'column_name', and 'description'.\n",
    "    use_best_matching_columns : bool, optional\n",
    "        A flag indicating whether to use only the best-matching columns (if True) or all columns in the table (if False). Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sql_prompt : FewShotPromptTemplate\n",
    "        A FewShotPromptTemplate object that can be used with an LLM to generate SQL queries.\n",
    "    \"\"\"\n",
    "    # Prepare table_info string based on the best matching table and columns\n",
    "    # table_info = f\"Table: {best_matching_table['table_name']} - {best_matching_table['description']}\\n\"\n",
    "    columns_info = \"Columns:\\n\"\n",
    "    has_date_column = False\n",
    "\n",
    "    # Determine which columns to use: best-matching or all columns\n",
    "    if use_best_matching_columns:\n",
    "        # If using best_matching_columns, use those provided (filtering columns_metadata based on matching logic)\n",
    "        columns_to_use = columns_metadata  # Assuming columns_metadata is already filtered\n",
    "    else:\n",
    "        # Use all columns for the given table from columns_metadata\n",
    "        columns_to_use = [col for col in columns_metadata if col[0] == best_matching_table['table_name']]\n",
    "\n",
    "    # Construct the columns_info string\n",
    "    for column in columns_to_use:\n",
    "        table_name, column_name, column_description = column\n",
    "        columns_info += f\"    Column: {column_name} - {column_description}\\n\"\n",
    "        if 'date' in column_name.lower():\n",
    "            has_date_column = True\n",
    "\n",
    "    # Create FewShot Prompt with instructions for handling most recent data\n",
    "    example_prompt = PromptTemplate.from_template(\"User input: {input}\\nSQL query: {query}\")\n",
    "\n",
    "    # Add a special instruction if the table has a date column\n",
    "    recent_data_instruction = (\n",
    "        \"If the user does not specify a date, retrieve the most recent data available by ordering the results \"\n",
    "        \"by the date column in descending order.\"\n",
    "    ) if has_date_column else \"\"\n",
    "\n",
    "    # Combine table_info and columns_info in the prompt\n",
    "    sql_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=(\n",
    "            \"You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. \"\n",
    "            \"Unless otherwise specified, do not return more than {top_k} rows.\\n\\n\"\n",
    "            \"Here is the relevant table information:\\n{table_info}\\n\\n\"\n",
    "            f\"{recent_data_instruction}\\n\\n\"\n",
    "            \"Below are a number of examples of questions and their corresponding SQL queries.\"\n",
    "        ),\n",
    "        suffix=\"User input: {input}\\nSQL query: \",\n",
    "        input_variables=[\"input\", \"table_info\", \"top_k\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return sql_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer_chain(llm):\n",
    "    # Define the prompt template with emphasis on including units, time-specific details, and using the latest data when time is not specified\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a knowledgeable assistant. Given the following user question and SQL result, answer the question accurately.\n",
    "        \n",
    "        Always ensure to:\n",
    "        1. Include appropriate units in your answer (e.g., Kwacha per kg, liters, etc.).\n",
    "        2. Specify the time period or date if the question implies or explicitly asks for it.\n",
    "        3. If the user does not specify a time, provide the most recent information available in the database and clearly state that this is the latest data.\n",
    "        4. if the SQL result has number with decimals, please round it \n",
    "\n",
    "        For example, if the user asks \"What's the price of Maize?\", your answer should include the price with the correct unit and mention that this is the most recent price, e.g., \"The most recent price of Maize is 60 Kwacha per kg.\"\n",
    "        If the user asks about a specific time period, such as \"What's the price of Maize for May 2024?\", include the time in your answer, e.g., \"The price of Maize in May 2024 is 60 Kwacha per kg.\"\n",
    "\n",
    "        Question: {question}\n",
    "        SQL Result: {result}\n",
    "        Answer: \"\"\"\n",
    "    )\n",
    "\n",
    "    return answer_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_chain(user_query, best_table_info, columns_info, best_columns=None, llm=None):\n",
    "    \"\"\"\n",
    "    Executes an SQL query generation chain using a language model (LLM) based on the user query, \n",
    "    best matching table, and columns information.\n",
    "\n",
    "    This function loads example SQL queries, creates an SQL prompt tailored to the best matching \n",
    "    table and its columns, and then executes a chain that generates and executes an SQL query. \n",
    "    The response is returned after processing the generated query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_query : str\n",
    "        The user's query for which an SQL query needs to be generated.\n",
    "    best_table_info : dict\n",
    "        A dictionary containing the best matching table information with 'table_name' and 'description' keys.\n",
    "    columns_info : list of tuples\n",
    "        A list of tuples containing columns metadata for the table. Each tuple includes 'table_name', \n",
    "        'column_name', and 'description'.\n",
    "    best_columns : list of tuples, optional\n",
    "        A list of tuples containing the best matching columns metadata, if available. If provided, \n",
    "        the SQL prompt will be generated using only these columns. Default is None.\n",
    "    llm : Any, optional\n",
    "        The language model (e.g., ChatOpenAI) to be used for generating the SQL query. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response : Any\n",
    "        The response from the executed SQL query chain, typically containing the results of the SQL query.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    response = run_sql_chain(\n",
    "        user_query=\"What is the price of maize?\",\n",
    "        best_table_info={\"table_name\": \"maize_prices\", \"description\": \"Contains maize price data\"},\n",
    "        columns_info=[(\"maize_prices\", \"price\", \"Price of maize\"), (\"maize_prices\", \"date\", \"Date of the price entry\")],\n",
    "        llm=ChatOpenAI()\n",
    "    )\n",
    "    print(response)\n",
    "    \"\"\"\n",
    "    # Load examples and create prompts\n",
    "    examples = load_sql_examples(file_path=FILE_SQL_EXAMPLES_EN)\n",
    "    \n",
    "    # Create SQL Query\n",
    "    if USE_BEST_MATCHING_COLUMNS and best_columns:\n",
    "        sql_prompt = create_sql_prompt(\n",
    "            examples=examples, \n",
    "            best_matching_table=best_table_info, \n",
    "            columns_metadata=best_columns, \n",
    "            use_best_matching_columns=True\n",
    "        )\n",
    "    else:\n",
    "        sql_prompt = create_sql_prompt(\n",
    "            examples=examples, \n",
    "            best_matching_table=best_table_info, \n",
    "            columns_metadata=columns_info\n",
    "        )\n",
    "\n",
    "    # Initialize LLM and other components\n",
    "    best_table = best_table_info['table_name']\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    db = SQLDatabase(engine=engine, ignore_tables=['table_metadata', 'column_metadata'])\n",
    "\n",
    "    execute_query = QuerySQLDataBaseTool(db=db)\n",
    "    write_query = create_sql_query_chain(llm, db, sql_prompt)\n",
    "\n",
    "    # Create the answer chain\n",
    "    answer_chain = create_answer_chain(llm)\n",
    "\n",
    "    # Put everything together\n",
    "    master_chain = (\n",
    "        RunnablePassthrough.assign(query=write_query).assign(\n",
    "            result=itemgetter(\"query\") | execute_query\n",
    "        )\n",
    "        | answer_chain\n",
    "    )\n",
    "\n",
    "    # Invoke the master chain and return the response\n",
    "    response = master_chain.invoke({\n",
    "        \"question\": user_query, \n",
    "        \"top_k\": 3,\n",
    "        \"table_info\": best_table\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sql_query(user_question, use_huggingface=False):\n",
    "    \"\"\"\n",
    "    Processes a user's question by generating and executing an SQL query using a language model (LLM). \n",
    "    Optionally, uses a Hugging Face model or defaults to OpenAI's GPT-3.5-turbo.\n",
    "\n",
    "    This function first initializes the appropriate LLM based on the `use_huggingface` flag. It then \n",
    "    retrieves metadata information, identifies the best matching table and relevant columns, and \n",
    "    executes the SQL query based on the processed information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_question : str\n",
    "        The user's question for which an SQL query needs to be generated and executed.\n",
    "    use_huggingface : bool, optional\n",
    "        A flag to determine whether to use a Hugging Face model instead of the default OpenAI model. \n",
    "        Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : Any\n",
    "        The output from the executed SQL query chain, typically containing the results of the SQL query.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    output = process_sql_query(\n",
    "        user_question=\"What is the price of maize?\",\n",
    "        use_huggingface=False\n",
    "    )\n",
    "    print(output)\n",
    "    \"\"\"\n",
    "    # Initialize LLM\n",
    "    # To Do: add Hugging Face LLM\n",
    "    if use_huggingface:\n",
    "        pass  # Hugging Face LLM initialization can be added here\n",
    "    else:\n",
    "        openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "   \n",
    "    # Retrieve the metadata info (tables and columns)\n",
    "    tables, columns = connect_to_database()\n",
    "\n",
    "    # Chain 1: Find the Best Table\n",
    "    best_table_chain, context = find_best_table_prompt(user_question, tables, columns, llm=openai_llm)\n",
    "    best_table_output_str = best_table_chain.run(**context)\n",
    "\n",
    "    # Convert the string output to a dictionary\n",
    "    try:\n",
    "        best_table_output = json.loads(best_table_output_str)['best_matching_table']\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: The output is not valid JSON.\")\n",
    "        best_table_output = None\n",
    "\n",
    "    # Chain 2: Find Relevant Columns\n",
    "    best_columns_chain, context = find_best_columns_prompt(user_question, best_table_output, columns, llm=openai_llm)\n",
    "    best_columns_output = best_columns_chain.run(**context)\n",
    "\n",
    "    # Retrieve result \n",
    "    output = run_sql_chain(\n",
    "        user_query=user_question, \n",
    "        best_table_info=best_table_output, \n",
    "        columns_info=columns, \n",
    "        best_columns=best_columns_output, \n",
    "        llm=openai_llm\n",
    "    )\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_embedding_distance(df, lan='English', \n",
    "                                     distance='cosine', ref_response_col='response-1'):\n",
    "\n",
    "\n",
    "    # Filter the dataframe to keep only instances for that language\n",
    "    df = df.query('language == @lan')\n",
    "    \n",
    "    # Add columns to keep LLM-response and eval score\n",
    "    df['llm_response'] = None\n",
    "    score_name = f\"score_{distance}\"\n",
    "    df[score_name] = None\n",
    "\n",
    "    \n",
    "    # Setup evaluator\n",
    "    if distance == \"cosine\":\n",
    "        dist_metric = EmbeddingDistance.COSINE\n",
    "    elif distance == \"euclidean\":\n",
    "        dist_metric = EmbeddingDistance.EUCLIDEAN\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings()\n",
    "    hf_evaluator = load_evaluator(\"embedding_distance\", distance_metric=dist_metric, \n",
    "                              embeddings=embedding_model)\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        reference_response = row[ref_response_col]\n",
    "        llm_response = process_sql_query(question)\n",
    "        print('LLM-Response', \"===>\", llm_response)\n",
    "        if not llm_response:\n",
    "            print('Blank response')\n",
    "        score = hf_evaluator.evaluate_strings(prediction=llm_response, \n",
    "                                              reference=reference_response)\n",
    "        df.loc[idx, 'llm_response'] = llm_response\n",
    "        df.loc[idx, score_name] = score['score']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_info = db.table_info\n",
    "# len(table_info)\n",
    "\n",
    "# df_eval = pd.read_csv(\"data/raw/eval-set.csv\")\n",
    "# df_res = evaluate_with_embedding_distance(df=df_eval, lan='English', distance='cosine')\n",
    "\n",
    "# question = \"Whats the price of Maize?\"\n",
    "# response = process_sql_query(user_question=question, llm=openai_chat_model)\n",
    "\n",
    "\n",
    "# create_sql_prompt(examples, best_matching_table, columns_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Full Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Set the logging level for the `httpx` logger to WARNING to suppress INFO logs\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# You can also suppress other loggers if necessary\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"langchain\").setLevel(logging.WARNING)\n",
    "\n",
    "# Set the logging level for langsmith.client to ERROR to suppress warnings\n",
    "logging.getLogger(\"langsmith.client\").setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress a specific warning\n",
    "warnings.filterwarnings(\"ignore\", \".*USER_AGENT environment variable not set.*\")\n",
    "\n",
    "from sql_chain import process_sql_query, get_latest_date\n",
    "\n",
    "from utils import translate_text_openai, classify_query_llm\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_date(db, commodity=None):\n",
    "    \"\"\"\n",
    "    Retrieves the most recent date from the commodity prices table in the database.\n",
    "    \n",
    "    If a commodity is specified, it retrieves the latest date for that commodity.\n",
    "    If no commodity is specified or if the specified commodity is not found, it retrieves the latest date across all commodities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    db : SQLDatabase\n",
    "        The database connection object.\n",
    "    commodity : str, optional\n",
    "        The name of the commodity to filter by. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The latest date as a string in the format 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if commodity:\n",
    "            query = f\"\"\"\n",
    "            SELECT MAX(collection_date) as latest_date\n",
    "            FROM commodity_prices\n",
    "            WHERE commodity = '{commodity}';\n",
    "            \"\"\"\n",
    "            result = db.run(query)\n",
    "            \n",
    "            # Check if the result contains a date\n",
    "            print(result)\n",
    "            latest_date = result[0]['latest_date'] if result else None\n",
    "\n",
    "            if latest_date:\n",
    "                return latest_date\n",
    "        \n",
    "        # If no commodity is provided or the query failed, retrieve the latest date without filtering by commodity\n",
    "        query = \"\"\"\n",
    "        SELECT MAX(collection_date) as latest_date\n",
    "        FROM commodity_prices;\n",
    "        \"\"\"\n",
    "        result = db.run(query)\n",
    "        \n",
    "        if result:\n",
    "            return result[0]['latest_date']\n",
    "        else:\n",
    "            raise ValueError(\"Failed to retrieve the latest date from the database.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving latest date: {e}\")\n",
    "        # Optionally, return a default value or raise an error\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(DATABASE_URL)\n",
    "db = SQLDatabase(engine=engine, ignore_tables=['table_metadata', 'column_metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error retrieving latest date: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "get_latest_date(db, commodity=\"Maize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_queries_en = []\n",
    "with open(\"sql_examples_en.json\", 'r') as file:\n",
    "        examples = json.load(file)\n",
    "        for item in examples:\n",
    "                valid_queries_en.append(item['input']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Hie\"\n",
    "classify_query_llm(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_valid_embeddings(valid_queries, embeddings):\n",
    "    \"\"\"\n",
    "    Prepare embeddings for a list of valid queries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    valid_queries : list of str\n",
    "        A list of valid queries that can be converted to SQL.\n",
    "    embeddings : Any\n",
    "        The embedding model used to generate embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of lists\n",
    "        A list of embeddings for the valid queries.\n",
    "    \"\"\"\n",
    "    return [embeddings.embed_query(query) for query in valid_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query_llm(user_query, llm=None):\n",
    "    \"\"\"\n",
    "    Classify the user query using an LLM based on examples of valid and invalid SQL-generating queries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_query : str\n",
    "        The user's query.\n",
    "    llm : Any, optional\n",
    "        The language model (e.g., ChatGPT, GPT-3.5) to be used for classification. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the query is classified as valid for SQL generation, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example queries\n",
    "    examples = [\n",
    "        {\"query\": \"What is the price of maize?\", \"classification\": \"Valid\"},\n",
    "        {\"query\": \"How much maize was produced last year?\", \"classification\": \"Valid\"},\n",
    "        {\"query\": \"Where can I find food?\", \"classification\": \"Valid\"},\n",
    "        {\"query\": \"Tell me a joke.\", \"classification\": \"Invalid\"},\n",
    "        {\"query\": \"Hello\", \"classification\": \"Invalid\"},\n",
    "        {\"query\": \"Bot\", \"classification\": \"Invalid\"}\n",
    "    ]\n",
    "\n",
    "    # Construct the prompt\n",
    "    example_prompts = \"\\n\".join([f'Query: \"{ex[\"query\"]}\"\\nClassification: {ex[\"classification\"]}' for ex in examples])\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        f\"\"\"\n",
    "        You are a knowledgeable assistant who can determine whether a query is valid for generating an SQL query or not. \n",
    "        Given the following examples, classify the new query accordingly.\n",
    "\n",
    "        Examples:\n",
    "        {example_prompts}\n",
    "\n",
    "        Now classify the following query:\n",
    "\n",
    "        Query: \"{{user_query}}\"\n",
    "        Classification (Valid/Invalid):\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Initialize the LLM if not provided\n",
    "    if not llm:\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Create the LLMChain for classification\n",
    "    classification_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Perform the classification\n",
    "    result = classification_chain.run({\"user_query\": user_query})\n",
    "\n",
    "    # Return True if classified as valid, otherwise False\n",
    "    return \"Valid\" in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_query = \"Hey\"\n",
    "is_valid = classify_query_llm(user_query)\n",
    "if is_valid:\n",
    "    print(\"Valid SQL-related query\")\n",
    "else:\n",
    "    print(\"Not a valid SQL-related query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_query = \"Thumba la chimanga ndi zingati pano?\"\n",
    "en_query = translate_text_openai(ny_query, \"Chichewa\", \"English\")\n",
    "valid = classify_query_llm(en_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sql_query(ny_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions = [\"What is the price of Maize in Rumphi\",\n",
    "                 \"Where can I find the cheapest maize?\",\n",
    "                    \"Which district harvested the most beans?\",\n",
    "                    \"How much is Maize in Zomba?\",\n",
    "                    \"Which district produced more Tobacco, Mchinji or Kasungu?\",\n",
    "                    \"Where can I get bananas?\", \"Kodi chimanga chotchipa ndingachipeze kuti?\",\n",
    "                    \"Ndi boma liti komwe anakolola nyemba zambiri?\",\n",
    "                    \"Ku Zomba chimanga akugulitsa pa bwanji?\",\n",
    "                    \"Kodi ndi boma liti anakolola chimanga chambiri pakati pa Lilongwe kapena Kasungu?\",\n",
    "                    \"Ndikuti ndingapeze mpunga wambiri?\"]\n",
    "\n",
    "for q in questions:\n",
    "    print()\n",
    "    print(\"QUESTION:\", q)\n",
    "    response= process_sql_query(q)\n",
    "    print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text_openai(\"Ndikuti ndingapeze mpunga wambiri?\", \"Chichewa\", \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = pd.read_csv(\"data/tables/prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_prices.query('Month_Name == \"May\" and Commodity ==\"Maize\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.Price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = pd.read_csv(\"data/raw/prices-maize.csv\")\n",
    "df_prices2 = df_prices.replace(\"#DIV/0!\", np.NaN)\n",
    "df_prices2['Price'] = df_prices2['Price'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices2.drop(columns=['EPA', 'Year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices2.query('Month == \"February\" and Market == \"Phalombe\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices2.query('Month == \"February\" and Market == \"Chikhwawa\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices2.query('Month == \"May\"').Price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
