{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chains import create_sql_query_chain, LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.evaluation import load_evaluator, EmbeddingDistance\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import psycopg2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = config(\"OPENAI_API_KEY\")\n",
    "DB_USER = config('DB_USER')\n",
    "DB_PASSWORD = config('DB_PASSWORD')\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '5432'\n",
    "DB_NAME = \"food_security\"\n",
    "SAMPLE_QUESTIONS = {\"low-birthweight\": \"Which region has the highest number of children born with low birth weights?\",\n",
    "                    \"vaccine_rates\": \"Which vaccine has the lowest vaccination percentage?\",\n",
    "                    \"vaccine_rates_all\": \"What percentage of children received all vaccines before 12 months\",\n",
    "                    \"kids_in_sch\": \"Whats average percentage of children who are in preschool\",\n",
    "                    \"vaccines\": \"which vaccines did children get in Tunisia?\"\n",
    "                    }\n",
    "                    \n",
    "# Create the database URL\n",
    "DATABASE_URL = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "FILE_SQL_EXAMPLES_EN = \"sql_examples_en.json\"\n",
    "USE_BEST_MATCHING_COLUMNS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt to Select best Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_database(database_url=DATABASE_URL):\n",
    "    \"\"\"Connects to a postgreSQL\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    database_url : String\n",
    "        postgreSQL database connection URL, by default DATABASE_URL\n",
    "    \"\"\"\n",
    "    # conn = psycopg2.connect(f\"dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD}\")\n",
    "    conn = psycopg2.connect(database_url)\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Query to get table names and column names\n",
    "    cur.execute(\"SELECT table_name, description FROM table_metadata\")\n",
    "    tables = cur.fetchall()\n",
    "\n",
    "    cur.execute(\"SELECT table_name, column_name, description FROM column_metadata\")\n",
    "    columns = cur.fetchall()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return tables, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_table_prompt(user_query, tables, columns, \n",
    "                           return_chain=True, llm=None):# Define the template for selecting the best table\n",
    "    template = \"\"\"\n",
    "    You are a database assistant. Given the following tables and columns with their descriptions, select the best table that matches the user's query.\n",
    "\n",
    "    Tables and Columns:\n",
    "    {table_info}\n",
    "\n",
    "    User Query:\n",
    "    {user_query}\n",
    "\n",
    "    Provide the output in the following JSON format:\n",
    "    {{\n",
    "        \"best_matching_table\": {{\n",
    "            \"table_name\": \"<best_table_name>\",\n",
    "            \"description\": \"<best_table_description>\"\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # Prepare the table_info string including descriptions for each table and its columns\n",
    "    table_info = \"\"\n",
    "    for table in tables:\n",
    "        table_name, table_description = table\n",
    "        table_info += f\"Table: {table_name} - {table_description}\\n\"\n",
    "        table_columns = [col for col in columns if col[0] == table_name]\n",
    "        for column in table_columns:\n",
    "            _, column_name, column_description = column\n",
    "            table_info += f\"    Column: {column_name} - {column_description}\\n\"\n",
    "        table_info += \"\\n\"\n",
    "\n",
    "    # Create the PromptTemplate\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"table_info\", \"user_query\"]\n",
    "    )\n",
    "\n",
    "    # Format the template \n",
    "    formatted_prompt = prompt_template.format(table_info=table_info, user_query=user_query)\n",
    "\n",
    "    if return_chain:\n",
    "        # Create the chain using the ChatOpenAI model and the PromptTemplate\n",
    "        chain = LLMChain(llm=llm,prompt=prompt_template)\n",
    "        return chain, {\"table_info\": table_info, \"user_query\": user_query}\n",
    "\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_info(table_name, columns):\n",
    "    columns_info = \"\"\n",
    "    for column in columns:\n",
    "        table, column_name, column_description = column\n",
    "        if table == table_name:\n",
    "            columns_info += f\"    Column: {column_name} - {column_description}\\n\"\n",
    "    return columns_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_columns_prompt(user_query, best_matching_table, columns, \n",
    "                       return_chain=True, llm=None):\n",
    "    # Define the template for selecting the relevant columns\n",
    "    column_template = \"\"\"\n",
    "    You are a database assistant. Given the following columns for the table '{table_name}', select the columns that are most relevant to the user's query.\n",
    "\n",
    "    Table Description: {table_description}\n",
    "\n",
    "    Columns:\n",
    "    {columns_info}\n",
    "\n",
    "    User Query:\n",
    "    {user_query}\n",
    "\n",
    "    Relevant Columns:\n",
    "    \"\"\"\n",
    "\n",
    "    columns_info = get_columns_info(best_matching_table[\"table_name\"], columns)\n",
    "\n",
    "    # Create the PromptTemplate for column selection\n",
    "    column_prompt_template = PromptTemplate(\n",
    "        template=column_template,\n",
    "        input_variables=[\"table_name\", \"table_description\", \"columns_info\", \"user_query\"]\n",
    "    )\n",
    "\n",
    "    # Example usage of the template with a user query\n",
    "    formatted_column_prompt = column_prompt_template.format(\n",
    "        table_name=best_matching_table[\"table_name\"],\n",
    "        table_description=best_matching_table[\"description\"],\n",
    "        columns_info=columns_info,\n",
    "        user_query=user_query\n",
    "    )\n",
    "\n",
    "    # Prepare the context for running the chain\n",
    "    context = {\n",
    "        \"table_name\": best_matching_table[\"table_name\"],\n",
    "        \"table_description\": best_matching_table[\"description\"],\n",
    "        \"columns_info\": columns_info,\n",
    "        \"user_query\": user_query}\n",
    "\n",
    "    if return_chain:\n",
    "        chain = LLMChain(llm=llm,prompt=column_prompt_template)\n",
    "        return chain, context\n",
    "\n",
    "    return formatted_column_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sql_examples(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_prompt(examples, best_matching_table, columns_metadata, \n",
    "                      use_best_matching_columns=False, llm=None):\n",
    "    \"\"\"\n",
    "    Creates a FewShotPromptTemplate for generating SQL queries based on table and column metadata.\n",
    "\n",
    "    This function generates a prompt template that includes detailed information about the table and its columns.\n",
    "    The generated prompt instructs a language model (LLM) to create a syntactically correct SQL query based on\n",
    "    user input. If the table contains a date column and the user does not specify a date, the prompt also instructs\n",
    "    the LLM to retrieve the most recent data available.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : list of dict\n",
    "        A list of example inputs and corresponding SQL queries. Each example should be a dictionary with 'input' and 'query' keys.\n",
    "    best_matching_table : dict\n",
    "        A dictionary containing the best matching table information with 'table_name' and 'description' keys.\n",
    "    columns_metadata : list of tuples\n",
    "        A list of tuples containing columns metadata. Each tuple should include 'table_name', 'column_name', and 'description'.\n",
    "    use_best_matching_columns : bool, optional\n",
    "        A flag indicating whether to use only the best-matching columns (if True) or all columns in the table (if False). Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sql_prompt : FewShotPromptTemplate\n",
    "        A FewShotPromptTemplate object that can be used with an LLM to generate SQL queries.\n",
    "    \"\"\"\n",
    "    # Prepare table_info string based on the best matching table and columns\n",
    "    # table_info = f\"Table: {best_matching_table['table_name']} - {best_matching_table['description']}\\n\"\n",
    "    columns_info = \"Columns:\\n\"\n",
    "    has_date_column = False\n",
    "\n",
    "    # Determine which columns to use: best-matching or all columns\n",
    "    if use_best_matching_columns:\n",
    "        # If using best_matching_columns, use those provided (filtering columns_metadata based on matching logic)\n",
    "        columns_to_use = columns_metadata  # Assuming columns_metadata is already filtered\n",
    "    else:\n",
    "        # Use all columns for the given table from columns_metadata\n",
    "        columns_to_use = [col for col in columns_metadata if col[0] == best_matching_table['table_name']]\n",
    "\n",
    "    # Construct the columns_info string\n",
    "    for column in columns_to_use:\n",
    "        table_name, column_name, column_description = column\n",
    "        columns_info += f\"    Column: {column_name} - {column_description}\\n\"\n",
    "        if 'date' in column_name.lower():\n",
    "            has_date_column = True\n",
    "\n",
    "    # Create FewShot Prompt with instructions for handling most recent data\n",
    "    example_prompt = PromptTemplate.from_template(\"User input: {input}\\nSQL query: {query}\")\n",
    "\n",
    "    # Add a special instruction if the table has a date column\n",
    "    recent_data_instruction = (\n",
    "        \"If the user does not specify a date, retrieve the most recent data available by ordering the results \"\n",
    "        \"by the date column in descending order.\"\n",
    "    ) if has_date_column else \"\"\n",
    "\n",
    "    # Combine table_info and columns_info in the prompt\n",
    "    sql_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=(\n",
    "            \"You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. \"\n",
    "            \"Unless otherwise specified, do not return more than {top_k} rows.\\n\\n\"\n",
    "            \"Here is the relevant table information:\\n{table_info}\\n\\n\"\n",
    "            f\"{recent_data_instruction}\\n\\n\"\n",
    "            \"Below are a number of examples of questions and their corresponding SQL queries.\"\n",
    "        ),\n",
    "        suffix=\"User input: {input}\\nSQL query: \",\n",
    "        input_variables=[\"input\", \"table_info\", \"top_k\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return sql_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer_chain(llm):\n",
    "    # Define the prompt template with emphasis on including units, time-specific details, and using the latest data when time is not specified\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a knowledgeable assistant. Given the following user question and SQL result, answer the question accurately.\n",
    "        \n",
    "        Always ensure to:\n",
    "        1. Include appropriate units in your answer (e.g., Kwacha per kg, liters, etc.).\n",
    "        2. Specify the time period or date if the question implies or explicitly asks for it.\n",
    "        3. If the user does not specify a time, provide the most recent information available in the database and clearly state that this is the latest data.\n",
    "\n",
    "        For example, if the user asks \"What's the price of Maize?\", your answer should include the price with the correct unit and mention that this is the most recent price, e.g., \"The most recent price of Maize is 60 Kwacha per kg.\"\n",
    "        If the user asks about a specific time period, such as \"What's the price of Maize for May 2024?\", include the time in your answer, e.g., \"The price of Maize in May 2024 is 60 Kwacha per kg.\"\n",
    "\n",
    "        Question: {question}\n",
    "        SQL Result: {result}\n",
    "        Answer: \"\"\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Create answer chain\n",
    "    return answer_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Whats the price of Maize in Rumphi?\"\n",
    "llm = ChatOpenAI()\n",
    "examples = load_sql_examples(file_path=FILE_SQL_EXAMPLES_EN)\n",
    "\n",
    "# Retrieve the metadata info (tables and columns)\n",
    "tables, columns = connect_to_database()\n",
    "\n",
    "# Chain 1: Find the Best Table\n",
    "best_table_chain, context = find_best_table_prompt(user_question, tables, \n",
    "                                                       columns, llm=llm)\n",
    "best_table_output_str = best_table_chain.run(**context)\n",
    "\n",
    "# Convert the string output to a dictionary\n",
    "try:\n",
    "    best_table_output = json.loads(best_table_output_str)['best_matching_table']\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: The output is not valid JSON.\")\n",
    "    best_table_output = None\n",
    "\n",
    "# Chain 2: Find Relevant Columns\n",
    "best_columns_chain, context = find_best_columns_prompt(user_question, best_table_output, \n",
    "                                                        columns, llm=llm)\n",
    "best_columns_output = best_columns_chain.run(**context)\n",
    "\n",
    "# Create SQL Query\n",
    "if USE_BEST_MATCHING_COLUMNS:\n",
    "        sql_prompt = create_sql_prompt(examples=examples, best_matching_table=best_table_output, \n",
    "                                   columns_metadata=best_columns_output, \n",
    "                                   use_best_matching_columns=True)\n",
    "else:\n",
    "        sql_prompt = create_sql_prompt(examples=examples, best_matching_table=best_table_output, \n",
    "                                   columns_metadata=columns)\n",
    "\n",
    "\n",
    "# Initialize LLM and other components\n",
    "best_table = best_table_output['table_name']\n",
    "engine = create_engine(DATABASE_URL)\n",
    "db = SQLDatabase(engine=engine, ignore_tables=['table_metadata', 'column_metadata'])\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "write_query = create_sql_query_chain(llm, db, sql_prompt)\n",
    "\n",
    "# Create the answer chain\n",
    "answer_chain = create_answer_chain(llm) | StrOutputParser()\n",
    "\n",
    "# Put everything together\n",
    "master_chain = (\n",
    "RunnablePassthrough.assign(query=write_query).assign(\n",
    "            result=itemgetter(\"query\") | execute_query\n",
    "        )\n",
    "        | answer_chain\n",
    "    )\n",
    "\n",
    "response = master_chain.invoke({\"question\":user_question, \n",
    "                                \"top_k\":3, \"table_info\":best_table}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(\"data/raw/eval-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dunstanmatekenya/Library/CloudStorage/GoogleDrive-dmatekenya@gmail.com/My Drive/phosoAI-whatsapp-app-aws/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5101afd7da46878f26839b7cfdead7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4628f0a43d5c4e83ac1262a18da584b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96cbfa817d7443ea3614d8fde00c4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ed8f26fb9144d2b247a4a50711fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92691f4725e0421682270caaa69cc2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8144342913946b096a8ba2ee57d6718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1831fb55e7d548afb04ce54241cdd38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102ab5c10efc49f7a15fd5d500c09350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e7a5506ed648ef913ede0acb9e2e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e12c7ad3a84d59b59956529b74f8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dunstanmatekenya/Library/CloudStorage/GoogleDrive-dmatekenya@gmail.com/My Drive/phosoAI-whatsapp-app-aws/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354d59dd9c0d4d7e8476fddc683a653a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup evaluator\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "hf_evaluator = load_evaluator(\"embedding_distance\", distance_metric=EmbeddingDistance.COSINE, \n",
    "                              embeddings=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_english_questions(df, prediction_chain):\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        reference = row['response-1']\n",
    "        prediction = master_chain.invoke\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/tables/prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633.265"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district = \"Balaka\"\n",
    "month = \"May\"\n",
    "crop = \"Maize\"\n",
    "query = f'District == \"{district}\" and Month_Name == \"{month}\" and Commodity == \"{crop}\"'\n",
    "\n",
    "result = df.query(query)\n",
    "result.Price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ny = pd.read_json(\"sql_examples_ny.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ny.to_csv(\"tmp_ny_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2704.912755319149"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('Commodity == \"Beans\" and Month_Name == \"May\"').Price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = pd.read_csv(\"data/tables/production.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Yield</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lilongwe</td>\n",
       "      <td>Maize</td>\n",
       "      <td>444440.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kasungu</td>\n",
       "      <td>Maize</td>\n",
       "      <td>318203.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dowa</td>\n",
       "      <td>Maize</td>\n",
       "      <td>259890.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mzimba</td>\n",
       "      <td>Maize</td>\n",
       "      <td>248415.253</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mchinji</td>\n",
       "      <td>Maize</td>\n",
       "      <td>244792.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dedza</td>\n",
       "      <td>Maize</td>\n",
       "      <td>227724.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ntchisi</td>\n",
       "      <td>Maize</td>\n",
       "      <td>185021.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Zomba</td>\n",
       "      <td>Maize</td>\n",
       "      <td>119523.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mangochi</td>\n",
       "      <td>Maize</td>\n",
       "      <td>104262.600</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chitipa</td>\n",
       "      <td>Maize</td>\n",
       "      <td>102319.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ntcheu</td>\n",
       "      <td>Maize</td>\n",
       "      <td>81248.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rumphi</td>\n",
       "      <td>Maize</td>\n",
       "      <td>77748.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nkhotakota</td>\n",
       "      <td>Maize</td>\n",
       "      <td>77164.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karonga</td>\n",
       "      <td>Maize</td>\n",
       "      <td>67998.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Machinga</td>\n",
       "      <td>Maize</td>\n",
       "      <td>62438.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Thyolo</td>\n",
       "      <td>Maize</td>\n",
       "      <td>60254.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chikwawa</td>\n",
       "      <td>Maize</td>\n",
       "      <td>55804.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Salima</td>\n",
       "      <td>Maize</td>\n",
       "      <td>53711.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Phalombe</td>\n",
       "      <td>Maize</td>\n",
       "      <td>52316.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balaka</td>\n",
       "      <td>Maize</td>\n",
       "      <td>43565.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nsanje</td>\n",
       "      <td>Maize</td>\n",
       "      <td>40221.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nkhata</td>\n",
       "      <td>Maize</td>\n",
       "      <td>39833.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mulanje</td>\n",
       "      <td>Maize</td>\n",
       "      <td>36175.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chiradzulu</td>\n",
       "      <td>Maize</td>\n",
       "      <td>35078.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blantyre</td>\n",
       "      <td>Maize</td>\n",
       "      <td>34369.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Neno</td>\n",
       "      <td>Maize</td>\n",
       "      <td>20054.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mwanza</td>\n",
       "      <td>Maize</td>\n",
       "      <td>18772.000</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Likoma</td>\n",
       "      <td>Maize</td>\n",
       "      <td>255.100</td>\n",
       "      <td>2023-2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      District   Crop       Yield     Season\n",
       "10    Lilongwe  Maize  444440.000  2023-2024\n",
       "8      Kasungu  Maize  318203.000  2023-2024\n",
       "6         Dowa  Maize  259890.000  2023-2024\n",
       "15      Mzimba  Maize  248415.253  2023-2024\n",
       "13     Mchinji  Maize  244792.000  2023-2024\n",
       "5        Dedza  Maize  227724.000  2023-2024\n",
       "21     Ntchisi  Maize  185021.000  2023-2024\n",
       "26       Zomba  Maize  119523.000  2023-2024\n",
       "12    Mangochi  Maize  104262.600  2023-2024\n",
       "4      Chitipa  Maize  102319.000  2023-2024\n",
       "20      Ntcheu  Maize   81248.000  2023-2024\n",
       "23      Rumphi  Maize   77748.000  2023-2024\n",
       "18  Nkhotakota  Maize   77164.000  2023-2024\n",
       "7      Karonga  Maize   67998.000  2023-2024\n",
       "11    Machinga  Maize   62438.000  2023-2024\n",
       "25      Thyolo  Maize   60254.000  2023-2024\n",
       "2     Chikwawa  Maize   55804.000  2023-2024\n",
       "24      Salima  Maize   53711.000  2023-2024\n",
       "22    Phalombe  Maize   52316.000  2023-2024\n",
       "0       Balaka  Maize   43565.000  2023-2024\n",
       "19      Nsanje  Maize   40221.000  2023-2024\n",
       "17      Nkhata  Maize   39833.000  2023-2024\n",
       "14     Mulanje  Maize   36175.000  2023-2024\n",
       "3   Chiradzulu  Maize   35078.000  2023-2024\n",
       "1     Blantyre  Maize   34369.000  2023-2024\n",
       "16        Neno  Maize   20054.000  2023-2024\n",
       "27      mwanza  Maize   18772.000  2023-2024\n",
       "9       Likoma  Maize     255.100  2023-2024"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod.query('Crop == \"Maize\"').sort_values(by=[\"Yield\", \"District\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_chain(user_query, best_table_info, columns_info, \n",
    "                  best_columns=None, llm=None):\n",
    "    \n",
    "    # Load examples and create prompts\n",
    "    examples = load_sql_examples(file_path=FILE_SQL_EXAMPLES_EN)\n",
    "    if USE_BEST_MATCHING_COLUMNS:\n",
    "        sql_prompt = create_sql_prompt(examples=examples, best_matching_table=best_table_info, \n",
    "                                   columns_metadata=columns_info, \n",
    "                                   use_best_matching_columns=True)\n",
    "    else:\n",
    "        sql_prompt = create_sql_prompt(examples=examples, best_matching_table=best_table_info, \n",
    "                                   columns_metadata=columns_info)\n",
    "    \n",
    "    # Initialize LLM and other components\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    db = SQLDatabase(engine=engine, ignore_tables=['table_metadata', 'column_metadata'])\n",
    "\n",
    "    execute_query = QuerySQLDataBaseTool(db=db)\n",
    "    write_query = create_sql_query_chain(llm, db, sql_prompt)\n",
    "\n",
    "    # Create the answer chain\n",
    "    answer_chain = create_answer_chain(llm)\n",
    "\n",
    "    # Put everything together\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(query=write_query).assign(\n",
    "            result=itemgetter(\"query\") | execute_query\n",
    "        )\n",
    "        | answer_chain\n",
    "    )\n",
    "\n",
    "    return chain.invoke({\"question\": \"{}\".format(user_question)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sql_query(user_question, llm=None):\n",
    "    \n",
    "    # LLM\n",
    "    if not llm:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Retrieve the metadata info (tables and columns)\n",
    "    tables, columns = connect_to_database()\n",
    "\n",
    "    # Chain 1: Find the Best Table\n",
    "    best_table_chain, context = find_best_table_prompt(user_question, tables, \n",
    "                                                       columns, llm=llm)\n",
    "    best_table_output_str = best_table_chain.run(**context)\n",
    "\n",
    "    # Convert the string output to a dictionary\n",
    "    try:\n",
    "        best_table_output = json.loads(best_table_output_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: The output is not valid JSON.\")\n",
    "        best_table_output = None\n",
    "\n",
    "    # Chain 2: Find Relevant Columns\n",
    "    table_name = best_table_output['best_matching_table']['table_name']\n",
    "    best_columns_chain, context = find_best_columns_prompt(user_question, best_table_output['best_matching_table'], \n",
    "                                                        columns, llm=llm)\n",
    "    best_columns_output = best_columns_chain.run(**context)\n",
    "    \n",
    "\n",
    "    response = run_sql_chain(user_question, best_table_output['best_matching_table'], \n",
    "                            columns_info, best_columns_output, llm=llm)\n",
    "    \n",
    "\n",
    "    return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Whats the price of Maize?\"\n",
    "openai_chat_model = ChatOpenAI()\n",
    "response = process_sql_query(user_question=question, llm=openai_chat_model)\n",
    "\n",
    "\n",
    "create_sql_prompt(examples, best_matching_table, columns_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.5)\n",
    "messages = [SystemMessage(content='''Act as a senior software engineer\n",
    "at a startup company.'''),\n",
    "HumanMessage(content='''Please can you provide a funny joke\n",
    "about software engineers?''')]\n",
    "response = chat.invoke(input=messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.5)\n",
    "messages = [SystemMessage(content='''You are a highly-skilled linguist and polyglot.\\\n",
    "              Identify the language of the user query'''),\n",
    "HumanMessage(content='''Dzina langa ndine Dunstan Matekenya''')]\n",
    "response = chat.invoke(input=messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x lists of messages, which is the same as [messages, messages]\n",
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "print(synchronous_llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Create a RunnableConfig with the desired concurrency limit:\n",
    "config = RunnableConfig(max_concurrency=5)\n",
    "\n",
    "# Call the .batch() method with the inputs and config:\n",
    "results = chat.batch([messages, messages], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import (SystemMessagePromptTemplate,\n",
    "ChatPromptTemplate)\n",
    "\n",
    "template = \"\"\"\n",
    "You are a creative consultant brainstorming names for businesses.\n",
    "\n",
    "You must follow the following principles:\n",
    "{principles}\n",
    "\n",
    "Please generate a numerical list of five catchy names for a start-up in the\n",
    "{industry} industry that deals with {context}?\n",
    "\n",
    "Here is an example output format:\n",
    "- Name1\n",
    "- Name2\n",
    "- Name3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt])\n",
    "\n",
    "chain = chat_prompt | model\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"industry\": \"medical\",\n",
    "    \"context\":'''creating AI solutions by automatically summarizing patient\n",
    "    records''',\n",
    "    \"principles\":'''1. Each name should be short and easy to\n",
    "    remember. 2. Each name should be easy to pronounce.\n",
    "    3. Each name should be unique and not already taken by another company.'''\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the template text\n",
    "template = '''You are a helpful assistant that translates {input_language}\n",
    "to {output_language}.'''\n",
    "\n",
    "# Create the PromptTemplate Object \n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = [\"input_language\", \"output_language\"]\n",
    ")\n",
    "\n",
    "# Convert base prompt into Chat System prompt \n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "# Create chat model\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Create message from user\n",
    "text_to_translate = HumanMessage(content=\"Chimanga chikupezeka kuti?\")\n",
    "\n",
    "# format the system message\n",
    "formatted_sys_message = system_message_prompt.format(\n",
    "    input_language=\"Chichewa\", output_language=\"English\")\n",
    "\n",
    "response = chat.invoke([formatted_sys_message, text_to_translate] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SystemMessagePromptTemplate.format_messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the system message template\n",
    "template = '''You are a helpful assistant that translates {input_language} to {output_language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_language\", \"output_language\"]\n",
    ")\n",
    "\n",
    "# Create the system message prompt\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "# Format the system message\n",
    "formatted_system_message = system_message_prompt.format(input_language=\"Chichewa\", output_language=\"English\")\n",
    "\n",
    "# Create a human message\n",
    "text_to_translate = HumanMessage(content=\"Chimanga chikupezeka kuti?\")\n",
    "\n",
    "# Initialize the chat model\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Call the invoke method with both the human message and the formatted system message\n",
    "response = chat.invoke([formatted_system_message, text_to_translate])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers \n",
    "Enables you to take outputs from an LLM and convert it into the format you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "temperature = 0.0\n",
    "\n",
    "class BusinessName(BaseModel):\n",
    "    name: str = Field(description=\"The name of the business\")\n",
    "    rating_score: float = Field(description='''The rating score of the\n",
    "    business. 0 is the worst, 10 is the best.''')\n",
    "\n",
    "class BusinessNames(BaseModel):\n",
    "    names: List[BusinessName] = Field(description='''A list\n",
    "    of busines names''')\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template:\n",
    "parser = PydanticOutputParser(pydantic_object=BusinessNames)\n",
    "\n",
    "principles = \"\"\"\n",
    "- The name must be easy to remember.\n",
    "- Use the {industry} industry and Company context to create an effective name.\n",
    "- The name must be easy to pronounce.\n",
    "- You must only return the name without any other text or characters.\n",
    "- Avoid returning full stops, \\n, or any other characters.\n",
    "- The maximum length of the name must be 10 characters.\n",
    "\"\"\"\n",
    "\n",
    "# Chat Model Output Parser:\n",
    "model = ChatOpenAI()\n",
    "template = \"\"\"Generate five business names for a new start-up company in the\n",
    "{industry} industry.\n",
    "You must follow the following principles: {principles}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIMessage(content='''Vous êtes un assistant utile qui traduit l'anglais en\n",
    "français.''', additional_kwargs={}, example=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "openai = OpenAI(openai_api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt_template.format(query=\"Which libraries and model providers offer LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0  # increase creativity/randomness of output\n",
    "\n",
    "print(openai(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "print(openai(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Define two variants of the prompt to test zero-shot\n",
    "# vs few-shot\n",
    "prompt_A = \"\"\"Product description: A pair of shoes that can\n",
    "fit any foot size.\n",
    "Seed words: adaptable, fit, omni-fit.\n",
    "Product names:\"\"\"\n",
    "\n",
    "prompt_B = \"\"\"Product description: A home milkshake maker.\n",
    "Seed words: fast, healthy, compact.\n",
    "Product names: HomeShaker, Fit Shaker, QuickShake, Shake\n",
    "Maker\n",
    "\n",
    "Product description: A watch that can tell accurate time in\n",
    "space.\n",
    "Seed words: astronaut, space-hardened, eliptical orbit\n",
    "Product names: AstroTime, SpaceGuard, Orbit-Accurate,\n",
    "EliptoTime.\n",
    "\n",
    "Product description: A pair of shoes that can fit any foot\n",
    "size.\n",
    "Seed words: adaptable, fit, omni-fit.\n",
    "Product names:\"\"\"\n",
    "\n",
    "test_prompts = [prompt_A, prompt_B]\n",
    "\n",
    "\n",
    "# Set your OpenAI key as an environment variable\n",
    "# https://platform.openai.com/api-keys\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # Default\n",
    ")\n",
    "\n",
    "def get_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Iterate through the prompts and get responses\n",
    "responses = []\n",
    "num_tests = 5\n",
    "\n",
    "for idx, prompt in enumerate(test_prompts):\n",
    "    # prompt number as a letter\n",
    "    var_name = chr(ord('A') + idx)\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        # Get a response from the model\n",
    "        response = get_response(prompt)\n",
    "\n",
    "        data = {\n",
    "            \"variant\": var_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response\n",
    "            }\n",
    "        responses.append(data)\n",
    "\n",
    "# Convert responses into a dataframe\n",
    "df = pd.DataFrame(responses)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "df.to_csv(\"responses.csv\", index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# load the responses.csv file\n",
    "df = pd.read_csv(\"responses.csv\")\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# df is your dataframe and 'response' is the column with the\n",
    "# text you want to test\n",
    "response_index = 0\n",
    "# add a new column to store feedback\n",
    "df['feedback'] = pd.Series(dtype='str')\n",
    "\n",
    "\n",
    "def update_response():\n",
    "    new_response = df.iloc[response_index]['response']\n",
    "    if pd.notna(new_response):\n",
    "        new_response = \"<p>\" + new_response + \"</p>\"\n",
    "    else:\n",
    "        new_response = \"<p>No response</p>\"\n",
    "    response.value = new_response\n",
    "    count_label.value = f\"Response: {response_index + 1}\"\n",
    "    count_label.value += f\"/{len(df)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_clicked(b):\n",
    "    global response_index\n",
    "    #  convert thumbs up / down to 1 / 0\n",
    "    user_feedback = 1 if b.description == \"\\U0001F44D\" else 0\n",
    "\n",
    "    # update the feedback column\n",
    "    df.at[response_index, 'feedback'] = user_feedback\n",
    "\n",
    "    response_index += 1\n",
    "    if response_index < len(df):\n",
    "        update_response()\n",
    "    else:\n",
    "        # save the feedback to a CSV file\n",
    "        df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "        print(\"A/B testing completed. Here's the results:\")\n",
    "        # Calculate score and num rows for each variant\n",
    "        summary_df = df.groupby('variant').agg(\n",
    "            count=('feedback', 'count'),\n",
    "            score=('feedback', 'mean')).reset_index()\n",
    "        print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = widgets.HTML()\n",
    "count_label = widgets.Label()\n",
    "\n",
    "update_response()\n",
    "\n",
    "thumbs_up_button = widgets.Button(description='\\U0001F44D')\n",
    "thumbs_up_button.on_click(on_button_clicked)\n",
    "\n",
    "thumbs_down_button = widgets.Button(\n",
    "    description='\\U0001F44E')\n",
    "thumbs_down_button.on_click(on_button_clicked)\n",
    "\n",
    "button_box = widgets.HBox([thumbs_down_button,\n",
    "thumbs_up_button])\n",
    "\n",
    "display(response, button_box, count_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# load the responses.csv file\n",
    "df = pd.read_csv(\"responses.csv\")\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# df is your dataframe and 'response' is the column with the\n",
    "# text you want to test\n",
    "response_index = 0\n",
    "# add a new column to store feedback\n",
    "df['feedback'] = pd.Series(dtype='str')\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global response_index\n",
    "    #  convert thumbs up / down to 1 / 0\n",
    "    user_feedback = 1 if b.description == \"\\U0001F44D\" else 0\n",
    "\n",
    "    # update the feedback column\n",
    "    df.at[response_index, 'feedback'] = user_feedback\n",
    "\n",
    "    response_index += 1\n",
    "    if response_index < len(df):\n",
    "        update_response()\n",
    "    else:\n",
    "        # save the feedback to a CSV file\n",
    "        df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "        print(\"A/B testing completed. Here's the results:\")\n",
    "        # Calculate score and num rows for each variant\n",
    "        summary_df = df.groupby('variant').agg(\n",
    "            count=('feedback', 'count'),\n",
    "            score=('feedback', 'mean')).reset_index()\n",
    "        print(summary_df)\n",
    "\n",
    "def update_response():\n",
    "    new_response = df.iloc[response_index]['response']\n",
    "    if pd.notna(new_response):\n",
    "        new_response = \"<p>\" + new_response + \"</p>\"\n",
    "    else:\n",
    "        new_response = \"<p>No response</p>\"\n",
    "    response.value = new_response\n",
    "    count_label.value = f\"Response: {response_index + 1}\"\n",
    "    count_label.value += f\"/{len(df)}\"\n",
    "\n",
    "response = widgets.HTML()\n",
    "count_label = widgets.Label()\n",
    "\n",
    "update_response()\n",
    "\n",
    "thumbs_up_button = widgets.Button(description='\\U0001F44D')\n",
    "thumbs_up_button.on_click(on_button_clicked)\n",
    "\n",
    "thumbs_down_button = widgets.Button(\n",
    "    description='\\U0001F44E')\n",
    "thumbs_down_button.on_click(on_button_clicked)\n",
    "\n",
    "button_box = widgets.HBox([thumbs_down_button,\n",
    "thumbs_up_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(response, button_box, count_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import LangDetectTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "     # Initialize the OpenAI API\n",
    "    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')\n",
    "\n",
    "    # Create the language detection tool\n",
    "    lang_detect_tool = LangDetectTool()\n",
    "\n",
    "    # Create a Tool object\n",
    "    lang_detect = Tool(\n",
    "        name=\"Language Detection\",\n",
    "        func=lang_detect_tool.run,\n",
    "        description=\"Useful for detecting the language of a given text.\"\n",
    "    )\n",
    "\n",
    "    # Use the tool to detect language\n",
    "    return lang_detect.run(text)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "def prompt_router(query):\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Similarity results=>\", most_similar)\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_router(\"Explain Pythagoras theorem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_object_with_metadata():\n",
    "    # Create the SQLAlchemy engine\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    metadata_obj = MetaData()\n",
    "    metadata_obj.reflect(bind=engine)\n",
    "\n",
    "    # Create a configured \"Session\" class\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    # Load custom metadata from the table_metadata and column_metadata tables\n",
    "    try:\n",
    "        table_metadata = session.execute(\"SELECT * FROM table_metadata\").fetchall()\n",
    "        column_metadata = session.execute(\"SELECT * FROM column_metadata\").fetchall()\n",
    "\n",
    "        # Add table metadata\n",
    "        for row in table_metadata:\n",
    "            print(row)\n",
    "            table_name = row['table_name']\n",
    "            description = row['description']\n",
    "            table = metadata_obj.tables.get(table_name)\n",
    "            table.info['description'] = description\n",
    "\n",
    "        # Add column metadata\n",
    "        for row in column_metadata:\n",
    "            table_name = row['table_name']\n",
    "            column_name = row['column_name']\n",
    "            description = row['description']\n",
    "            table = metadata_obj.tables.get(table_name)\n",
    "            column = table.columns.get(column_name)\n",
    "            column.info['description'] = description\n",
    "    finally:\n",
    "        session.close()\n",
    "    db = SQLDatabase(engine=engine, metadata=metadata_obj, ignore_tables=['table_metadata', 'column_metadata'])\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities_price = ['Maize', 'Rice', 'Soya beans', 'Beans', 'Cow peas', 'Groundnuts']\n",
    "crop_estimates = ['Maize', 'Beans', 'Cow peas', 'Dolichus beans ', 'Soy beans',\n",
    "       'Ground beans', 'Paprika', 'Rice', 'Pigeon peas', 'Grams',\n",
    "       'Sesame ', 'Field peas', 'Velvet beans', 'Chick peas', 'Wheat',\n",
    "       'Millet', 'Sorghum ', 'Groundnuts', 'Cassava', 'Sweet potatoes',\n",
    "       'Potatoes', 'Tobacco', 'Flue cured', 'Sunflower ', 'Chillies',\n",
    "       'Cotton ', 'Bananas', 'Mangoes', 'Oranges', 'Tangerines', 'Coffee',\n",
    "       'Pineapples', 'Guava', 'Pawpaws', 'Peaches', 'Lemons',\n",
    "       'Grape fruits', 'Apples', 'Avocado pear', 'Macademia', 'Tomatoes',\n",
    "       'Onions', 'Cabbage', 'Egg plants', 'Okra', 'Cucumber']\n",
    "price_estimates_key_words = [\"price\", \"cheap\", \"produce\", \n",
    "                                 \"buy\", \"sell\", \"sale\", \"find\"]\n",
    "all_kw = [i.lower() for i in set(commodities_price+crop_estimates+price_estimates_key_words)]\n",
    "print(all_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def load_examples(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "def format_translation_examples(examples_file, source_language, target_language):\n",
    "    examples = load_examples(examples_file)\n",
    "    key = f\"{source_language}-{target_language}\"\n",
    "    if key in examples:\n",
    "        return \"\\n\".join([f\"{source_language}: {ex[source_language]}\\n{target_language}: {ex[target_language]}\" \n",
    "                          for ex in examples[key]])\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def translate_with_openai(text, src_lan, dest_lan):\n",
    "    # Create a ChatOpenAI instance\n",
    "    chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Get and format translation examples\n",
    "    formated_examples = format_translation_examples(\"./translation_examples.json\", source_language=src_lan, \n",
    "                                target_language=dest_lan)\n",
    "    # Create a system message with examples\n",
    "    system_template = \"\"\"You are a professional translator. Your task is to translate {src_lan} to {dest_lan}.\n",
    "        Here are a few examples:\n",
    "\n",
    "        {examples}\n",
    "\n",
    "        Now, translate the following text:\"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "    # Create a human message for the actual translation request\n",
    "    human_template = \"{text}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    # Combine the prompts\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    # Create an LLMChain for translation\n",
    "    translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
    "    \n",
    "    return translation_chain.run({\n",
    "        \"source_language\": src_lan,\n",
    "        \"target_language\": dest_lan,\n",
    "        \"examples\": formated_examples,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
    "# Create a system message with examples\n",
    "system_template = \"\"\"You are a professional translator. Your task is to translate {source_language} to {target_language}.\n",
    "Here are a few examples:\n",
    "\n",
    "{examples}\n",
    "\n",
    "Now, translate the following text:\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Create a human message for the actual translation request\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Combine the prompts\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Create an LLMChain for translation\n",
    "translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, source_language, target_language):\n",
    "    formated_examples = format_translation_examples(\"./translation_examples.json\", source_language, \n",
    "                                target_language)\n",
    "    return translation_chain.run({\n",
    "        \"source_language\": source_language,\n",
    "        \"target_language\": target_language,\n",
    "        \"examples\": formated_examples,\n",
    "        \"text\": text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(\"Mpunga ukugulitsidwa pa mtengo wanji?\", \"Chichewa\", \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_translation_examples(examples_file=\"./translation_examples.json\", \n",
    "                            source_language=\"Chichewa\", target_language=\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_examples(examples, source_language=\"Chichewa\", target_language=\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_examples = examples[\"Chichewa-English\"]\n",
    "\n",
    "for item in relevant_examples:\n",
    "    print(item)\n",
    "    \"\\n\".join([f\"{source_language}: {ex['source']}\\n{target_language}: {ex['target']}\" \n",
    "                          for ex in examples[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a ChatOpenAI instance\n",
    "chat_model = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create a system message with examples\n",
    "system_template = \"\"\"You are a professional translator. Your task is to translate {source_language} to {target_language}.\n",
    "Here are a few examples:\n",
    "\n",
    "{source_language}: Mtedza ukugulitsidwa pabwanji?\n",
    "{target_language}: Whats the price of groundnuts?\n",
    "\n",
    "{source_language}: Chimanga chikupezeka kuti?\n",
    "{target_language}: Where can I find maize?\n",
    "\n",
    "{source_language}: Ndikuti nyemba zikutchipa?\n",
    "{target_language}: Where can I find beans at cheap price?\n",
    "\n",
    "{source_language}: Chimanga chili pabwanji pano?\n",
    "{target_language}: Whats the price of maize now?\n",
    "\n",
    "{source_language}: Ku Dowa chimanga chili pa bwanji?\n",
    "{target_language}: Whats the price of maize in Dowa?\n",
    "\n",
    "{source_language}: Kodi ndi boma liti anakolola chimanga chambiri pakati pa Lilongwe kapena Kasungu?\n",
    "{target_language}: Which district produced more maize: Lilongwe or Kasungu?\n",
    "\n",
    "{source_language}: Kodi chimanga chili pa bwanji ku Rumphi?\n",
    "{target_language}: How much is maize per Kg in Rumphi?\n",
    "\n",
    "{source_language}: Mpunga ukugulitsidwa ndalama zingati ku Lilongwe?\n",
    "{target_language}: Whats the price of rice in Lilongwe?\n",
    "\n",
    "{source_language}: Mtedza otchipa ukupezeka mboma liti?\n",
    "{target_language}: Which district has the cheap price for groundnuts?\n",
    "\n",
    "{source_language}: Chimanga chambiri chikupezeka kuti?\n",
    "{target_language}: Where can I find maize?\n",
    "\n",
    "{source_language}: Ndi boma liti komwe anakolola chimanga chambiri?\n",
    "{target_language}: Which district harvested large quantities of maize?\n",
    "\n",
    "{source_language}: Ndi mbeu zanji anakolola bwino ku Rumphi?\n",
    "{target_language}: Which crops produced the most yields in Rumphi\n",
    "\n",
    "{source_language}: Soya ali pabwanji?\n",
    "{target_language}: Whats the price of soya?\n",
    "\n",
    "{source_language}: Mtedza otchipa ndingaupeze kuti?\n",
    "{target_language}: Where can I find groundnuts at reasonable price?\n",
    "\n",
    "\n",
    "Now, translate the following text:\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Create a human message for the actual translation request\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Combine the prompts\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Create an LLMChain for translation\n",
    "translation_chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, source_language, target_language):\n",
    "    return translation_chain.run({\n",
    "        \"source_language\": source_language,\n",
    "        \"target_language\": target_language,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "# Example usage\n",
    "source_text = \"Mtedza ndingaupeze kuti?\"\n",
    "source_language = \"Chichewa\"\n",
    "target_language = \"English\"\n",
    "\n",
    "translated_text = translate_text(source_text, source_language, target_language)\n",
    "\n",
    "print(f\"{source_language}: {source_text}\")\n",
    "print(f\"{target_language}: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI instance\n",
    "llm = OpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY, model=\"gpt-4\")\n",
    "\n",
    "# Create a prompt template for translation\n",
    "translation_template = PromptTemplate(\n",
    "    input_variables=[\"source_language\", \"target_language\", \"text\"],\n",
    "    template=\"Translate the following {source_language} text to {target_language}: {text}\"\n",
    ")\n",
    "\n",
    "# Create an LLMChain for translation\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_template)\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, source_language, target_language):\n",
    "    return translation_chain.run({\n",
    "        \"source_language\": source_language,\n",
    "        \"target_language\": target_language,\n",
    "        \"text\": text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_text = \"Chimanga chili pa bwanji ku Malawi?\"\n",
    "source_language = \"Chichewa\"\n",
    "target_language = \"English\"\n",
    "\n",
    "translated_text = translate_text(source_text, source_language, target_language)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, source_language=\"English\", target_language=\"Chichewa\"):\n",
    "\n",
    "    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')\n",
    "    # Create a template for the translation\n",
    "    translation_template = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following {source_language} text to {target_language}: {text}\"\n",
    ")\n",
    "\n",
    "    # Create a chain with the LLM and the translation template\n",
    "    translation_chain = LLMChain(llm=llm, prompt=translation_template)\n",
    "\n",
    "    translation = translation_chain.run({\n",
    "        'text': text,\n",
    "        'source_language': source_language,\n",
    "        'target_language': target_language\n",
    "    })\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = translate_text(text=\"cheap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def translate_text(text, source_language=\"English\", target_language=\"Chichewa\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful assistant that translates {source_language} to {target_language}.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate the following text from {source_language} to {target_language}:\\n\\n{text}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    translation = response['choices'][0]['message']['content']\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(text=\"cheap\", source_language=\"English\", target_language=\"Chichewa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "def translate_text(text, source_language=\"en\", target_language=\"ny\"):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, src=source_language, dest=target_language)\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = create_db_object_with_metadata()\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "# chain = create_sql_query_chain(llm, db)\n",
    "# response = chain.invoke({\"question\": \"{}\".format(SAMPLE_QUESTIONS[\"low-birthweight\"])})\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlalchemy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples = [\n",
    "    {\"input\": \"Which region has the highest number of children born with low birth weights?\", \n",
    "     \"query\": \"SELECT * FROM tab4711 ORDER BY number_children DESC LIMIT 1;\",},\n",
    "\n",
    "     {\"input\": \"Which region has the highest percentage of children born with low birth weights?\", \n",
    "     \"query\": \"SELECT * FROM tab4711 ORDER BY percentage_below_2500g DESC LIMIT 1;\",\n",
    "     },\n",
    "\n",
    "     {\"input\": \"How many children received all vaccines before 12 months?\", \n",
    "     \"query\": \"SELECT vacc_b4_12months FROM tab501 WHERE vacc_category = 'All vaccinations';\"},\n",
    "\n",
    "     {\"input\": \"Which region has the lowest rates in preschool for children?\", \n",
    "     \"query\": \"SELECT * FROM tab9011 ORDER BY percentage_children_sch ASC LIMIT 1;\",},\n",
    "\n",
    "     {\"input\": \"Whats the average literacy rate among young women in Tunisia?\",\n",
    "      \"query\": \"SELECT AVG(percentage_literate) AS avg_literacy_rate FROM tab971;\",},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "write_query = create_sql_query_chain(llm, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "answer = answer_prompt | llm | StrOutputParser()\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"{}\".format(SAMPLE_QUESTIONS['vaccine_rates_all'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"User input: {input}\\nSQL query: {query}\")\n",
    "sql_prompt = FewShotPromptTemplate(\n",
    "    examples=examples[:5],\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. Unless otherwise specificed, do not return more than {top_k} rows.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\",\n",
    "    suffix=\"User input: {input}\\nSQL query: \",\n",
    "    input_variables=[\"input\", \"top_k\", \"table_info\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db, verbose=True)\n",
    "write_query = create_sql_query_chain(llm, db, sql_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "answer = answer_prompt | llm | StrOutputParser()\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"{}\".format(SAMPLE_QUESTIONS[\"vaccines\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
